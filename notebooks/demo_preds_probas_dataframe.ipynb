{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably not good to get into the habit of ignorning all of the RuntimeWarnings\n",
    "# but it keeps things cleaner for now\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from coremdlr.datasets import WellLoader, FaciesDataset\n",
    "from coremdlr.models import FeaturePredictor, LambdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Well:  205-21b-3  from  /home/ross/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Header section Parameter regexp=~P was not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding NaN log:  SP\n",
      "Adding NaN log:  DTS\n",
      "Feature shapes:  [('depth', (3842,)), ('top', (3842,)), ('base', (3842,)), ('pseudoGR', (3842, 32, 1)), ('logs', (3842, 11))]\n",
      "Loading Well:  204-24a-6  from  /home/ross/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Header section Parameter regexp=~P was not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding NaN log:  SP\n",
      "Adding NaN log:  DTS1\n",
      "Adding NaN log:  DTS2\n",
      "Feature shapes:  [('depth', (13006,)), ('top', (13006,)), ('base', (13006,)), ('pseudoGR', (13006, 32, 1)), ('logs', (13006, 11))]\n",
      "Loading Well:  204-20-6a  from  /home/ross/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Header section Parameter regexp=~P was not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding NaN log:  SP\n",
      "Adding NaN log:  DTS1\n",
      "Adding NaN log:  DTS2\n",
      "Feature shapes:  [('depth', (3542,)), ('top', (3542,)), ('base', (3542,)), ('pseudoGR', (3542, 32, 1)), ('logs', (3542, 11))]\n",
      "Loading Well:  204-19-6  from  /home/ross/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Header section Parameter regexp=~P was not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding NaN log:  DTS\n",
      "Feature shapes:  [('depth', (1884,)), ('top', (1884,)), ('base', (1884,)), ('pseudoGR', (1884, 32, 1)), ('logs', (1884, 11))]\n"
     ]
    }
   ],
   "source": [
    "# Note: for `reduce_function` to work as written, we need single-channel pGR\n",
    "\n",
    "fdset = FaciesDataset([\"205-21b-3\", \"204-24a-6\", \"204-20-6a\", ],\n",
    "                    test_wells=[\"204-19-6\"],\n",
    "                    features=[\"pseudoGR\", \"logs\"],\n",
    "                    pseudoGR_args={'features':['mean'], 'per_channel':False},\n",
    "                    label_resolution=32)\n",
    "\n",
    "fdset.load_or_generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudoGR': array([[[-1.43032357],\n",
       "         [-1.44530056],\n",
       "         [-1.44356918],\n",
       "         ...,\n",
       "         [-1.43711217],\n",
       "         [-1.43515969],\n",
       "         [-1.43175723]],\n",
       " \n",
       "        [[-1.43999973],\n",
       "         [-1.43694975],\n",
       "         [-1.42513606],\n",
       "         ...,\n",
       "         [-1.28338159],\n",
       "         [-1.27688571],\n",
       "         [-1.2691924 ]],\n",
       " \n",
       "        [[-1.25704702],\n",
       "         [-1.25234698],\n",
       "         [-1.25405517],\n",
       "         ...,\n",
       "         [-0.87033114],\n",
       "         [-0.84970345],\n",
       "         [-0.84871894]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-1.12578242],\n",
       "         [-1.05124625],\n",
       "         [-0.93005796],\n",
       "         ...,\n",
       "         [-0.48612563],\n",
       "         [-0.462301  ],\n",
       "         [-0.45574232]],\n",
       " \n",
       "        [[-0.43443013],\n",
       "         [-0.41625653],\n",
       "         [-0.41264419],\n",
       "         ...,\n",
       "         [-0.13868142],\n",
       "         [-0.09966698],\n",
       "         [-0.05836392]],\n",
       " \n",
       "        [[-0.07205497],\n",
       "         [-0.10052565],\n",
       "         [-0.06141172],\n",
       "         ...,\n",
       "         [-0.16115686],\n",
       "         [-0.13511807],\n",
       "         [-0.11393803]]]),\n",
       " 'logs': array([[-0.75023092,  0.        , -0.5883227 , ...,  0.        ,\n",
       "         -0.10760129,  0.55107293],\n",
       "        [-0.74827576,  0.        , -0.58813111, ...,  0.        ,\n",
       "         -0.1006304 ,  0.56120278],\n",
       "        [-0.7463206 ,  0.        , -0.58793953, ...,  0.        ,\n",
       "         -0.0936595 ,  0.57133264],\n",
       "        ...,\n",
       "        [-0.60622567,  0.        , -1.34991166, ..., -0.91765007,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.5942961 ,  0.        , -1.34587436, ..., -0.92112896,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.58236654,  0.        , -1.34183706, ..., -0.92460786,\n",
       "          0.        ,  0.        ]]),\n",
       " 'depth': array([1812.02945125, 1812.03482128, 1812.04019131, ..., 2200.13752307,\n",
       "        2200.1428931 , 2200.14826313]),\n",
       " 'top': array([1812.02685014, 1812.03222017, 1812.0375902 , ..., 2200.13492197,\n",
       "        2200.140292  , 2200.14566202]),\n",
       " 'base': array([1812.03222017, 1812.0375902 , 1812.04296023, ..., 2200.140292  ,\n",
       "        2200.14566202, 2200.15103205]),\n",
       " 'boundary_idxs': [0, 3842, 16848, 20390]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdset.X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "from scipy.stats import mstats\n",
    "\n",
    "\n",
    "# for balanced training and log_loss computation\n",
    "train_sample_weights = compute_sample_weight('balanced', fdset.y_train)\n",
    "test_sample_weights = compute_sample_weight('balanced', fdset.y_test)\n",
    "\n",
    "# feat_names = ['mean', 'median', 'hmean', 'gmean', 'var', 'IF_0', 'IF_1', 'Chi2', 'p-val']\n",
    "feat_names = ['mean', 'median', 'hmean', 'gmean', 'var', 'IF_0', 'IF_1']\n",
    "\n",
    "def reduce_function(x):\n",
    "    \n",
    "    feats = []\n",
    "    # Clean up `x`\n",
    "    x = np.squeeze(x) \n",
    "    x = np.ma.masked_invalid(x)\n",
    "    x = np.ma.masked_less_equal(x, 0.1)\n",
    "    \n",
    "    # central tendency\n",
    "    feats.append(np.mean(x, axis=-1))\n",
    "    feats.append(np.median(x, axis=-1))\n",
    "    feats.append(mstats.hmean(x, axis=-1))\n",
    "    feats.append(mstats.gmean(x, axis=-1))\n",
    "    \n",
    "    # dispersion\n",
    "    feats.append(np.var(x, axis=-1))\n",
    "    ideal_fourths = mstats.idealfourths(x, axis=-1)\n",
    "    feats.append(ideal_fourths[:, 0])\n",
    "    feats.append(ideal_fourths[:, 1])\n",
    "    \n",
    "    # TODO: curvature\n",
    "\n",
    "    x_feats = np.array(feats).T\n",
    "    \n",
    "    return x_feats\n",
    "\n",
    "\n",
    "feat_model_args = {\n",
    "    # NOTE: key needs to be feature name AND feature must be specified in model_args\n",
    "    'pseudoGR': {\n",
    "        'model' : 'LambdaModel',\n",
    "        'model_args' : {\n",
    "            'feature' : 'pseudoGR',\n",
    "            'lambda_fn' : reduce_function\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# NOTE: change `gpu_id` if your machine is different\n",
    "XGB_SEARCH_SPACE = {\n",
    "    'model_type' : 'XGB',\n",
    "    'max_depth' : scope.int(hp.quniform('max_depth', 3, 10, 1)),\n",
    "    'learning_rate' : hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators' : scope.int(hp.quniform('n_estimators', 10, 1000, 1)),\n",
    "    'objective' : 'multi:softprob',\n",
    "    'n_jobs' : 2,\n",
    "    'gamma' : hp.uniform('gamma', 0, 0.5),\n",
    "    'subsample' : hp.uniform('subsample', 0.3, 1),\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "    'colsample_bylevel' : 1,\n",
    "    'reg_alpha' : 0,                                    # L1 penalty\n",
    "    'reg_lambda' : hp.uniform('reg_lambda', 0.1, 10),   # L2 penalty\n",
    "    'tree_method' : 'gpu_hist',\n",
    "    #'gpu_id' : 0\n",
    "}\n",
    "\n",
    "\n",
    "def train_xgb_model(model_config):\n",
    "    \n",
    "    xgb_predictor = FeaturePredictor(fdset, \n",
    "                                     model_args=model_config, \n",
    "                                     feature_model_args=feat_model_args)\n",
    "    \n",
    "    test_acc = xgb_predictor.fit(fdset, verbose=False, sample_weight=train_sample_weights)\n",
    "    \n",
    "    y_pred = xgb_predictor.predict(fdset.X_test)\n",
    "    \n",
    "    # Note: have to specify labels here sometimes\n",
    "    print('F1 score:', f1_score(fdset.y_test, y_pred, labels=list(range(5)), average='macro'))\n",
    "    \n",
    "    return log_loss(fdset.y_test, xgb_predictor.predict_proba(fdset.X_test),\n",
    "                    labels=list(range(5)), sample_weight=test_sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for feature:                         \n",
      "pseudoGR                                            \n",
      "F1 score:                                           \n",
      "0.25185189887950343                                 \n",
      "Training model for feature:                                                 \n",
      "pseudoGR                                                                    \n",
      "F1 score:                                                                   \n",
      "0.2280621336912208                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.23629801055059135                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.23208934454381383                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.23062207692568565                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.23160320438403023                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.2370606266796635                                                           \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.22741841899487306                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                    \n",
      "F1 score:                                                                   \n",
      "0.24081644200674912                                                         \n",
      "Training model for feature:                                                 \n",
      "pseudoGR                                                                    \n",
      "F1 score:                                                                   \n",
      "0.2405899443977937                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.2288413415477193                                                           \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.22447961907367997                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.23280012470301387                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.2312241916605599                                                           \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.23641224444061676                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.2370842878586396                                                           \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.2392947885837339                                                           \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.2310139535143564                                                           \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.22892179388440206                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.2428663065778419                                                           \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.23244729216759535                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.2353255257521671                                                           \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.23127513083692253                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.23933182181459248                                                          \n",
      "Training model for feature:                                                  \n",
      "pseudoGR                                                                     \n",
      "F1 score:                                                                    \n",
      "0.23278787464083944                                                          \n",
      "100%|██████████| 25/25 [04:10<00:00, 10.03s/it, best loss: 1.376799421948054]\n"
     ]
    }
   ],
   "source": [
    "best_params = hyperopt.fmin(\n",
    "    fn=train_xgb_model,\n",
    "    space=XGB_SEARCH_SPACE,\n",
    "    algo=hyperopt.rand.suggest,\n",
    "    max_evals=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8014369069545624,\n",
       " 'gamma': 0.056814642055057374,\n",
       " 'learning_rate': 0.09204883242094557,\n",
       " 'max_depth': 5.0,\n",
       " 'n_estimators': 147.0,\n",
       " 'reg_lambda': 3.135589972892407,\n",
       " 'subsample': 0.8449117868256566}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for feature:  pseudoGR\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-74d673985789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxgb_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeaturePredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_model_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_model_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxgb_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/python/coremdlr/coremdlr/models/feature_predictor.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, **fit_args)\u001b[0m\n\u001b[1;32m    103\u001b[0m         return self.evaluate(dataset.X_test, dataset.y_test,\n\u001b[1;32m    104\u001b[0m                             \u001b[0mprint_report\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'verbose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                             save_report=fit_args.get('save_report'))\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/python/coremdlr/coremdlr/models/feature_predictor.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, X, y, print_report, save_report)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprint_report\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msave_report\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mcls_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprint_report\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nTotal accuracy Score : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/core-dev/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1877\u001b[0m             )\n\u001b[1;32m   1878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "# Error below is because test well doesn't have any 'oilstained'\n",
    "# I don't really want to fix this everywhere, can we just use test sets with every class?\n",
    "\n",
    "params = {**XGB_SEARCH_SPACE, \n",
    "          **best_params, \n",
    "          **{'max_depth': int(best_params['max_depth']), 'n_estimators': int(best_params['n_estimators'])}}\n",
    "                              \n",
    "xgb_predictor = FeaturePredictor(fdset, model_args=params, feature_model_args=feat_model_args)\n",
    "xgb_predictor.fit(fdset, verbose=True)\n",
    "\n",
    "imps = list(zip(feat_names, xgb_predictor.model.feature_importances_))\n",
    "imps.sort(key = lambda p: p[1])\n",
    "print()\n",
    "[print(pair) for pair in imps[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(xgb_predictor.predict(fdset.X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.predict_proba(fdset.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = xgb_predictor.save_preds(fdset.test_well_names[0])\n",
    "# ABOVE USES UNSCALED DATA! FIX THAT!\n",
    "\n",
    "df['y_pred'] = xgb_predictor.predict(fdset.X_test)\n",
    "df[['proba_0','proba_1','proba_2','proba_3']] = xgb_predictor.predict_proba(fdset.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='top', y='GR', figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='top', y='pseudoGR', figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='top', y='pseudoGR', kind='scatter', figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts\n",
    "\n",
    "Maybe we should do a little more work on filling in or masking the pseudo gamma. It's computed from the min-max normalized image (hence the 0-1 range), but maybe we should also standardize it after computing (probably less important for XGB model, but may help quite a bit for the networks which currently have comparable performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['normed_pGR'] = (df.pseudoGR - df.pseudoGR.mean()) / df.pseudoGR.std()\n",
    "df.plot(x='top', y=['normed_pGR','GR'], figsize=(15,5), ylim=[-2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at regression estimate / error / confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df['regression_error'] = (df.regression - df.y_true.astype(float)).abs()\n",
    "sns.scatterplot(x='regression_error', y='confidence', hue='y_true', data=df)\n",
    "\n",
    "#df.plot(x='regression_error', y='confidence', kind='scatter',  figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df['abs_class_distance'] = (df.y_true - df.y_pred).abs()\n",
    "sns.violinplot(x='abs_class_distance', y='confidence', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class_distance'] = (df.y_true - df.y_pred)\n",
    "sns.violinplot(x='class_distance', y='confidence', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices\n",
    "\n",
    "**NOTE**: probably going to move the analysis visualization functions into a different submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from coremdlr.facies.models_utils import make_confusion_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = fdset.classes\n",
    "cm = confusion_matrix(df.y_true.values, df.y_pred.values)\n",
    "\n",
    "# updated function below to have no grid by default\n",
    "make_confusion_fig(cm, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_fig(cm, classes, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
