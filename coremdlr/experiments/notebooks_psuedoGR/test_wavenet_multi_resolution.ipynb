{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't work very well. Might be worth mentioning though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from coremdlr.viz import model_plots, CorePlotter\n",
    "from coremdlr.models import NetworkModel\n",
    "from coremdlr.datasets import FaciesDataset\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet_model_args = {\n",
    "    'feature' : 'pseudoGR',\n",
    "    'network' : 'wavenet',\n",
    "    'summary' : False,\n",
    "    'sequence_size' : 128,\n",
    "    'network_args' : {\n",
    "        'num_blocks' : 20,\n",
    "        'block_filters' : 128,\n",
    "        'residual_filters' : 64,\n",
    "        'output_resolution' : 32,\n",
    "        'dropout_rate' : 0.5,\n",
    "    },\n",
    "    'optimizer_args' : {\n",
    "        'optimizer' : 'Adam',\n",
    "    },\n",
    "    'loss' : {'categorical_crossentropy': 0.25,\n",
    "              'ordinal_squared_error': 0.75}\n",
    "}\n",
    "\n",
    "CHKPT_FNAME = 'temp_best_weights.h5'\n",
    "\n",
    "wavenet_fit_args = {\n",
    "    'batch_size' : 16,\n",
    "    'epochs' : 20,\n",
    "    'class_weighted' : True\n",
    "}\n",
    "\n",
    "well_names = [\"205-21b-3\", \"204-20-6a\",\"204-20-1Z\", \"204-24a-6\", \"204-19-6\"]\n",
    "test_well_names = [\"204-19-6\", \"204-24a-6\", \"204-20-6a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (15363,)), ('top', (15363,)), ('base', (15363,)), ('pseudoGR', (15363, 8, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (15491,)), ('top', (15491,)), ('base', (15491,)), ('pseudoGR', (15491, 8, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (7670,)), ('top', (7670,)), ('base', (7670,)), ('pseudoGR', (7670, 8, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (52020,)), ('top', (52020,)), ('base', (52020,)), ('pseudoGR', (52020, 8, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (7540,)), ('top', (7540,)), ('base', (7540,)), ('pseudoGR', (7540, 8, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 512, 4)\n",
      "Epoch 1/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 1.3680 - acc: 0.2677\n",
      "Epoch 00001: val_loss improved from inf to 1.50676, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 28s 1s/step - loss: 1.3960 - acc: 0.2580 - val_loss: 1.5068 - val_acc: 0.1909\n",
      "Epoch 2/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8778 - acc: 0.3535\n",
      "Epoch 00002: val_loss improved from 1.50676 to 0.81385, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 8s 371ms/step - loss: 0.8691 - acc: 0.3572 - val_loss: 0.8138 - val_acc: 0.4973\n",
      "Epoch 3/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6722 - acc: 0.5210\n",
      "Epoch 00003: val_loss improved from 0.81385 to 0.61940, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 8s 368ms/step - loss: 0.6643 - acc: 0.5206 - val_loss: 0.6194 - val_acc: 0.6296\n",
      "Epoch 4/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6511 - acc: 0.5551\n",
      "Epoch 00004: val_loss did not improve from 0.61940\n",
      "22/22 [==============================] - 8s 361ms/step - loss: 0.6558 - acc: 0.5466 - val_loss: 0.6528 - val_acc: 0.5617\n",
      "Epoch 5/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6254 - acc: 0.5468\n",
      "Epoch 00005: val_loss did not improve from 0.61940\n",
      "22/22 [==============================] - 8s 360ms/step - loss: 0.6346 - acc: 0.5418 - val_loss: 0.6575 - val_acc: 0.6023\n",
      "Epoch 6/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6285 - acc: 0.5528\n",
      "Epoch 00006: val_loss did not improve from 0.61940\n",
      "22/22 [==============================] - 8s 362ms/step - loss: 0.6363 - acc: 0.5524 - val_loss: 0.6232 - val_acc: 0.5928\n",
      "Epoch 7/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6098 - acc: 0.5770\n",
      "Epoch 00007: val_loss improved from 0.61940 to 0.58227, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 8s 373ms/step - loss: 0.6103 - acc: 0.5775 - val_loss: 0.5823 - val_acc: 0.6110\n",
      "Epoch 8/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5689 - acc: 0.6060\n",
      "Epoch 00008: val_loss improved from 0.58227 to 0.56624, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 8s 371ms/step - loss: 0.5644 - acc: 0.6065 - val_loss: 0.5662 - val_acc: 0.6222\n",
      "Epoch 9/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5797 - acc: 0.6164\n",
      "Epoch 00009: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 362ms/step - loss: 0.5756 - acc: 0.6133 - val_loss: 0.6550 - val_acc: 0.4925\n",
      "Epoch 10/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5770 - acc: 0.5840\n",
      "Epoch 00010: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 362ms/step - loss: 0.5824 - acc: 0.5784 - val_loss: 0.5967 - val_acc: 0.6070\n",
      "Epoch 11/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5700 - acc: 0.6196\n",
      "Epoch 00011: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 366ms/step - loss: 0.5722 - acc: 0.6183 - val_loss: 0.7482 - val_acc: 0.5628\n",
      "Epoch 12/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5649 - acc: 0.6122\n",
      "Epoch 00012: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 368ms/step - loss: 0.5704 - acc: 0.6110 - val_loss: 0.6217 - val_acc: 0.5709\n",
      "Epoch 13/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5491 - acc: 0.6198\n",
      "Epoch 00013: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 374ms/step - loss: 0.5546 - acc: 0.6203 - val_loss: 0.6062 - val_acc: 0.6180\n",
      "Epoch 14/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5549 - acc: 0.6335\n",
      "Epoch 00014: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 375ms/step - loss: 0.5582 - acc: 0.6297 - val_loss: 0.6027 - val_acc: 0.5861\n",
      "Epoch 15/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5524 - acc: 0.6234\n",
      "Epoch 00015: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 378ms/step - loss: 0.5486 - acc: 0.6251 - val_loss: 0.6502 - val_acc: 0.5564\n",
      "Epoch 16/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5326 - acc: 0.6519\n",
      "Epoch 00016: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 379ms/step - loss: 0.5451 - acc: 0.6507 - val_loss: 0.5916 - val_acc: 0.5968\n",
      "Epoch 17/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5233 - acc: 0.6527\n",
      "Epoch 00017: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 380ms/step - loss: 0.5187 - acc: 0.6527 - val_loss: 0.6221 - val_acc: 0.5790\n",
      "Epoch 18/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5328 - acc: 0.6589\n",
      "Epoch 00018: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 377ms/step - loss: 0.5231 - acc: 0.6621 - val_loss: 0.6393 - val_acc: 0.5725\n",
      "Epoch 19/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5315 - acc: 0.6446\n",
      "Epoch 00019: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 379ms/step - loss: 0.5307 - acc: 0.6454 - val_loss: 0.6192 - val_acc: 0.5509\n",
      "Epoch 20/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5069 - acc: 0.6609\n",
      "Epoch 00020: val_loss did not improve from 0.56624\n",
      "22/22 [==============================] - 8s 378ms/step - loss: 0.5070 - acc: 0.6611 - val_loss: 0.6009 - val_acc: 0.5906\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (15363,)), ('top', (15363,)), ('base', (15363,)), ('pseudoGR', (15363, 8, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (15491,)), ('top', (15491,)), ('base', (15491,)), ('pseudoGR', (15491, 8, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (7670,)), ('top', (7670,)), ('base', (7670,)), ('pseudoGR', (7670, 8, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (7540,)), ('top', (7540,)), ('base', (7540,)), ('pseudoGR', (7540, 8, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (52020,)), ('top', (52020,)), ('base', (52020,)), ('pseudoGR', (52020, 8, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 512, 4)\n",
      "Epoch 1/20\n",
      "10/11 [==========================>...] - ETA: 1s - loss: 1.1925 - acc: 0.3298\n",
      "Epoch 00001: val_loss improved from inf to 1.62610, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 32s 3s/step - loss: 1.2088 - acc: 0.3413 - val_loss: 1.6261 - val_acc: 0.3011\n",
      "Epoch 2/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.7266 - acc: 0.4394\n",
      "Epoch 00002: val_loss improved from 1.62610 to 1.08490, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 7s 611ms/step - loss: 0.7135 - acc: 0.4428 - val_loss: 1.0849 - val_acc: 0.2018\n",
      "Epoch 3/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5923 - acc: 0.5095\n",
      "Epoch 00003: val_loss improved from 1.08490 to 0.94535, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 7s 612ms/step - loss: 0.5958 - acc: 0.5047 - val_loss: 0.9454 - val_acc: 0.2185\n",
      "Epoch 4/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5019 - acc: 0.5282\n",
      "Epoch 00004: val_loss improved from 0.94535 to 0.92209, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 7s 614ms/step - loss: 0.4882 - acc: 0.5330 - val_loss: 0.9221 - val_acc: 0.2907\n",
      "Epoch 5/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4505 - acc: 0.5765\n",
      "Epoch 00005: val_loss did not improve from 0.92209\n",
      "11/11 [==============================] - 7s 592ms/step - loss: 0.4386 - acc: 0.5808 - val_loss: 0.9841 - val_acc: 0.4872\n",
      "Epoch 6/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4259 - acc: 0.5958\n",
      "Epoch 00006: val_loss did not improve from 0.92209\n",
      "11/11 [==============================] - 6s 587ms/step - loss: 0.4157 - acc: 0.5949 - val_loss: 0.9336 - val_acc: 0.5126\n",
      "Epoch 7/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4016 - acc: 0.6286\n",
      "Epoch 00007: val_loss improved from 0.92209 to 0.89532, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 7s 620ms/step - loss: 0.4190 - acc: 0.6203 - val_loss: 0.8953 - val_acc: 0.2885\n",
      "Epoch 8/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3963 - acc: 0.6150\n",
      "Epoch 00008: val_loss did not improve from 0.89532\n",
      "11/11 [==============================] - 7s 591ms/step - loss: 0.4048 - acc: 0.6182 - val_loss: 0.9157 - val_acc: 0.5195\n",
      "Epoch 9/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3745 - acc: 0.6359\n",
      "Epoch 00009: val_loss did not improve from 0.89532\n",
      "11/11 [==============================] - 7s 599ms/step - loss: 0.3808 - acc: 0.6398 - val_loss: 0.9486 - val_acc: 0.5743\n",
      "Epoch 10/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3807 - acc: 0.6350\n",
      "Epoch 00010: val_loss did not improve from 0.89532\n",
      "11/11 [==============================] - 7s 592ms/step - loss: 0.3704 - acc: 0.6448 - val_loss: 0.9822 - val_acc: 0.5816\n",
      "Epoch 11/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3623 - acc: 0.6664\n",
      "Epoch 00011: val_loss did not improve from 0.89532\n",
      "11/11 [==============================] - 7s 599ms/step - loss: 0.3594 - acc: 0.6684 - val_loss: 1.0674 - val_acc: 0.5285\n",
      "Epoch 12/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3396 - acc: 0.6769\n",
      "Epoch 00012: val_loss improved from 0.89532 to 0.88593, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 7s 619ms/step - loss: 0.3447 - acc: 0.6753 - val_loss: 0.8859 - val_acc: 0.4859\n",
      "Epoch 13/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3533 - acc: 0.6539\n",
      "Epoch 00013: val_loss did not improve from 0.88593\n",
      "11/11 [==============================] - 7s 593ms/step - loss: 0.3738 - acc: 0.6469 - val_loss: 0.9341 - val_acc: 0.3768\n",
      "Epoch 14/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3762 - acc: 0.6496\n",
      "Epoch 00014: val_loss did not improve from 0.88593\n",
      "11/11 [==============================] - 7s 599ms/step - loss: 0.3765 - acc: 0.6506 - val_loss: 0.9973 - val_acc: 0.5675\n",
      "Epoch 15/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3634 - acc: 0.6549\n",
      "Epoch 00015: val_loss did not improve from 0.88593\n",
      "11/11 [==============================] - 7s 600ms/step - loss: 0.3646 - acc: 0.6633 - val_loss: 1.0158 - val_acc: 0.4231\n",
      "Epoch 16/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3583 - acc: 0.6629\n",
      "Epoch 00016: val_loss did not improve from 0.88593\n",
      "11/11 [==============================] - 7s 596ms/step - loss: 0.3538 - acc: 0.6645 - val_loss: 0.9673 - val_acc: 0.5215\n",
      "Epoch 17/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3289 - acc: 0.6773\n",
      "Epoch 00017: val_loss did not improve from 0.88593\n",
      "11/11 [==============================] - 7s 603ms/step - loss: 0.3255 - acc: 0.6765 - val_loss: 0.9802 - val_acc: 0.5658\n",
      "Epoch 18/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3238 - acc: 0.6804\n",
      "Epoch 00018: val_loss did not improve from 0.88593\n",
      "11/11 [==============================] - 7s 605ms/step - loss: 0.3265 - acc: 0.6792 - val_loss: 0.9012 - val_acc: 0.5397\n",
      "Epoch 19/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3123 - acc: 0.6989\n",
      "Epoch 00019: val_loss did not improve from 0.88593\n",
      "11/11 [==============================] - 7s 617ms/step - loss: 0.3063 - acc: 0.7007 - val_loss: 0.9894 - val_acc: 0.4008\n",
      "Epoch 20/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3151 - acc: 0.7146\n",
      "Epoch 00020: val_loss did not improve from 0.88593\n",
      "11/11 [==============================] - 7s 620ms/step - loss: 0.3164 - acc: 0.7137 - val_loss: 0.9304 - val_acc: 0.4877\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (15363,)), ('top', (15363,)), ('base', (15363,)), ('pseudoGR', (15363, 8, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (7670,)), ('top', (7670,)), ('base', (7670,)), ('pseudoGR', (7670, 8, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (52020,)), ('top', (52020,)), ('base', (52020,)), ('pseudoGR', (52020, 8, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (7540,)), ('top', (7540,)), ('base', (7540,)), ('pseudoGR', (7540, 8, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (15491,)), ('top', (15491,)), ('base', (15491,)), ('pseudoGR', (15491, 8, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 512, 4)\n",
      "Epoch 1/20\n",
      "19/20 [===========================>..] - ETA: 1s - loss: 1.5584 - acc: 0.1723\n",
      "Epoch 00001: val_loss improved from inf to 0.64559, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 39s 2s/step - loss: 1.5449 - acc: 0.1712 - val_loss: 0.6456 - val_acc: 0.4242\n",
      "Epoch 2/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.7887 - acc: 0.4138\n",
      "Epoch 00002: val_loss improved from 0.64559 to 0.45918, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 8s 403ms/step - loss: 0.7832 - acc: 0.4183 - val_loss: 0.4592 - val_acc: 0.4969\n",
      "Epoch 3/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.7023 - acc: 0.5313\n",
      "Epoch 00003: val_loss did not improve from 0.45918\n",
      "20/20 [==============================] - 8s 390ms/step - loss: 0.6902 - acc: 0.5382 - val_loss: 0.4627 - val_acc: 0.4359\n",
      "Epoch 4/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6700 - acc: 0.5485\n",
      "Epoch 00004: val_loss improved from 0.45918 to 0.43336, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 8s 403ms/step - loss: 0.6641 - acc: 0.5487 - val_loss: 0.4334 - val_acc: 0.4356\n",
      "Epoch 5/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6344 - acc: 0.5523\n",
      "Epoch 00005: val_loss did not improve from 0.43336\n",
      "20/20 [==============================] - 8s 390ms/step - loss: 0.6477 - acc: 0.5561 - val_loss: 0.5490 - val_acc: 0.4206\n",
      "Epoch 6/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6349 - acc: 0.5800\n",
      "Epoch 00006: val_loss did not improve from 0.43336\n",
      "20/20 [==============================] - 8s 393ms/step - loss: 0.6387 - acc: 0.5839 - val_loss: 0.4397 - val_acc: 0.4413\n",
      "Epoch 7/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6149 - acc: 0.6224\n",
      "Epoch 00007: val_loss did not improve from 0.43336\n",
      "20/20 [==============================] - 8s 393ms/step - loss: 0.6119 - acc: 0.6268 - val_loss: 0.5699 - val_acc: 0.5238\n",
      "Epoch 8/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6487 - acc: 0.5758\n",
      "Epoch 00008: val_loss improved from 0.43336 to 0.38039, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 8s 410ms/step - loss: 0.6540 - acc: 0.5670 - val_loss: 0.3804 - val_acc: 0.5990\n",
      "Epoch 9/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5913 - acc: 0.6490\n",
      "Epoch 00009: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 392ms/step - loss: 0.5937 - acc: 0.6456 - val_loss: 0.4165 - val_acc: 0.4971\n",
      "Epoch 10/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5919 - acc: 0.6457\n",
      "Epoch 00010: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 393ms/step - loss: 0.5871 - acc: 0.6416 - val_loss: 0.4093 - val_acc: 0.5136\n",
      "Epoch 11/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5681 - acc: 0.6584\n",
      "Epoch 00011: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 393ms/step - loss: 0.5663 - acc: 0.6604 - val_loss: 0.4015 - val_acc: 0.5263\n",
      "Epoch 12/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5726 - acc: 0.6676\n",
      "Epoch 00012: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 401ms/step - loss: 0.5678 - acc: 0.6676 - val_loss: 0.4397 - val_acc: 0.5157\n",
      "Epoch 13/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5754 - acc: 0.6584\n",
      "Epoch 00013: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 404ms/step - loss: 0.5659 - acc: 0.6584 - val_loss: 0.3988 - val_acc: 0.5292\n",
      "Epoch 14/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5501 - acc: 0.6670\n",
      "Epoch 00014: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 408ms/step - loss: 0.5644 - acc: 0.6603 - val_loss: 0.4008 - val_acc: 0.5175\n",
      "Epoch 15/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6100 - acc: 0.6404\n",
      "Epoch 00015: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 406ms/step - loss: 0.6151 - acc: 0.6401 - val_loss: 0.4115 - val_acc: 0.5392\n",
      "Epoch 16/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5473 - acc: 0.6740\n",
      "Epoch 00016: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 409ms/step - loss: 0.5469 - acc: 0.6671 - val_loss: 0.4146 - val_acc: 0.5274\n",
      "Epoch 17/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5599 - acc: 0.6732\n",
      "Epoch 00017: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 412ms/step - loss: 0.5489 - acc: 0.6710 - val_loss: 0.4231 - val_acc: 0.4794\n",
      "Epoch 18/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5372 - acc: 0.6775\n",
      "Epoch 00018: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 410ms/step - loss: 0.5393 - acc: 0.6732 - val_loss: 0.4184 - val_acc: 0.5078\n",
      "Epoch 19/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5634 - acc: 0.6689\n",
      "Epoch 00019: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 413ms/step - loss: 0.5624 - acc: 0.6660 - val_loss: 0.3930 - val_acc: 0.5230\n",
      "Epoch 20/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5304 - acc: 0.6746\n",
      "Epoch 00020: val_loss did not improve from 0.38039\n",
      "20/20 [==============================] - 8s 409ms/step - loss: 0.5262 - acc: 0.6796 - val_loss: 0.3980 - val_acc: 0.4862\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (3842,)), ('top', (3842,)), ('base', (3842,)), ('pseudoGR', (3842, 32, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (3873,)), ('top', (3873,)), ('base', (3873,)), ('pseudoGR', (3873, 32, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1917,)), ('top', (1917,)), ('base', (1917,)), ('pseudoGR', (1917, 32, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (13006,)), ('top', (13006,)), ('base', (13006,)), ('pseudoGR', (13006, 32, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1884,)), ('top', (1884,)), ('base', (1884,)), ('pseudoGR', (1884, 32, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 128, 4)\n",
      "Epoch 1/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 1.6718 - acc: 0.2144\n",
      "Epoch 00001: val_loss improved from inf to 1.97538, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 43s 2s/step - loss: 1.7055 - acc: 0.2107 - val_loss: 1.9754 - val_acc: 0.1540\n",
      "Epoch 2/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8611 - acc: 0.3576\n",
      "Epoch 00002: val_loss improved from 1.97538 to 0.62308, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 8s 381ms/step - loss: 0.8713 - acc: 0.3638 - val_loss: 0.6231 - val_acc: 0.5586\n",
      "Epoch 3/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7271 - acc: 0.4737\n",
      "Epoch 00003: val_loss did not improve from 0.62308\n",
      "22/22 [==============================] - 8s 366ms/step - loss: 0.7262 - acc: 0.4760 - val_loss: 0.7726 - val_acc: 0.5787\n",
      "Epoch 4/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6438 - acc: 0.5057\n",
      "Epoch 00004: val_loss improved from 0.62308 to 0.59363, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 8s 383ms/step - loss: 0.6473 - acc: 0.5025 - val_loss: 0.5936 - val_acc: 0.5790\n",
      "Epoch 5/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6102 - acc: 0.5951\n",
      "Epoch 00005: val_loss did not improve from 0.59363\n",
      "22/22 [==============================] - 8s 367ms/step - loss: 0.6007 - acc: 0.6038 - val_loss: 0.6088 - val_acc: 0.5876\n",
      "Epoch 6/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6121 - acc: 0.5529\n",
      "Epoch 00006: val_loss did not improve from 0.59363\n",
      "22/22 [==============================] - 8s 368ms/step - loss: 0.6059 - acc: 0.5527 - val_loss: 0.5939 - val_acc: 0.5982\n",
      "Epoch 7/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6094 - acc: 0.5726\n",
      "Epoch 00007: val_loss improved from 0.59363 to 0.56780, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 8s 385ms/step - loss: 0.6110 - acc: 0.5743 - val_loss: 0.5678 - val_acc: 0.6183\n",
      "Epoch 8/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5698 - acc: 0.6255\n",
      "Epoch 00008: val_loss improved from 0.56780 to 0.56541, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 8s 385ms/step - loss: 0.5651 - acc: 0.6301 - val_loss: 0.5654 - val_acc: 0.5826\n",
      "Epoch 9/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5625 - acc: 0.6263\n",
      "Epoch 00009: val_loss did not improve from 0.56541\n",
      "22/22 [==============================] - 8s 371ms/step - loss: 0.5539 - acc: 0.6300 - val_loss: 0.5725 - val_acc: 0.6038\n",
      "Epoch 10/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5359 - acc: 0.6291\n",
      "Epoch 00010: val_loss did not improve from 0.56541\n",
      "22/22 [==============================] - 8s 372ms/step - loss: 0.5431 - acc: 0.6291 - val_loss: 0.5906 - val_acc: 0.5938\n",
      "Epoch 11/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5516 - acc: 0.6231\n",
      "Epoch 00011: val_loss improved from 0.56541 to 0.55580, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 394ms/step - loss: 0.5441 - acc: 0.6265 - val_loss: 0.5558 - val_acc: 0.6323\n",
      "Epoch 12/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5372 - acc: 0.6307\n",
      "Epoch 00012: val_loss did not improve from 0.55580\n",
      "22/22 [==============================] - 8s 381ms/step - loss: 0.5549 - acc: 0.6277 - val_loss: 0.5973 - val_acc: 0.5946\n",
      "Epoch 13/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5599 - acc: 0.6405\n",
      "Epoch 00013: val_loss did not improve from 0.55580\n",
      "22/22 [==============================] - 8s 385ms/step - loss: 0.5613 - acc: 0.6370 - val_loss: 0.6388 - val_acc: 0.5854\n",
      "Epoch 14/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5242 - acc: 0.6096\n",
      "Epoch 00014: val_loss did not improve from 0.55580\n",
      "22/22 [==============================] - 8s 386ms/step - loss: 0.5479 - acc: 0.6033 - val_loss: 0.5933 - val_acc: 0.6016\n",
      "Epoch 15/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5319 - acc: 0.6427\n",
      "Epoch 00015: val_loss did not improve from 0.55580\n",
      "22/22 [==============================] - 8s 385ms/step - loss: 0.5369 - acc: 0.6398 - val_loss: 0.6923 - val_acc: 0.5427\n",
      "Epoch 16/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5511 - acc: 0.6319\n",
      "Epoch 00016: val_loss did not improve from 0.55580\n",
      "22/22 [==============================] - 8s 385ms/step - loss: 0.5442 - acc: 0.6348 - val_loss: 0.6607 - val_acc: 0.5943\n",
      "Epoch 17/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5691 - acc: 0.6035\n",
      "Epoch 00017: val_loss did not improve from 0.55580\n",
      "22/22 [==============================] - 8s 385ms/step - loss: 0.5632 - acc: 0.6086 - val_loss: 0.6741 - val_acc: 0.6038\n",
      "Epoch 18/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5308 - acc: 0.6478\n",
      "Epoch 00018: val_loss did not improve from 0.55580\n",
      "22/22 [==============================] - 8s 386ms/step - loss: 0.5261 - acc: 0.6484 - val_loss: 0.8446 - val_acc: 0.4975\n",
      "Epoch 19/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5398 - acc: 0.6440\n",
      "Epoch 00019: val_loss did not improve from 0.55580\n",
      "22/22 [==============================] - 8s 386ms/step - loss: 0.5438 - acc: 0.6410 - val_loss: 0.6146 - val_acc: 0.5977\n",
      "Epoch 20/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5188 - acc: 0.6494\n",
      "Epoch 00020: val_loss did not improve from 0.55580\n",
      "22/22 [==============================] - 9s 387ms/step - loss: 0.5186 - acc: 0.6493 - val_loss: 0.6694 - val_acc: 0.5865\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (3842,)), ('top', (3842,)), ('base', (3842,)), ('pseudoGR', (3842, 32, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (3873,)), ('top', (3873,)), ('base', (3873,)), ('pseudoGR', (3873, 32, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1917,)), ('top', (1917,)), ('base', (1917,)), ('pseudoGR', (1917, 32, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1884,)), ('top', (1884,)), ('base', (1884,)), ('pseudoGR', (1884, 32, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (13006,)), ('top', (13006,)), ('base', (13006,)), ('pseudoGR', (13006, 32, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 128, 4)\n",
      "Epoch 1/20\n",
      "10/11 [==========================>...] - ETA: 1s - loss: 2.0068 - acc: 0.3085\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30678, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 48s 4s/step - loss: 1.9836 - acc: 0.2922 - val_loss: 2.3068 - val_acc: 0.1873\n",
      "Epoch 2/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.8036 - acc: 0.4294\n",
      "Epoch 00002: val_loss improved from 2.30678 to 1.14733, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 7s 668ms/step - loss: 0.7726 - acc: 0.4367 - val_loss: 1.1473 - val_acc: 0.3100\n",
      "Epoch 3/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.5257 - acc: 0.5200\n",
      "Epoch 00003: val_loss improved from 1.14733 to 0.96624, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 7s 671ms/step - loss: 0.5285 - acc: 0.5212 - val_loss: 0.9662 - val_acc: 0.2828\n",
      "Epoch 4/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4772 - acc: 0.5715\n",
      "Epoch 00004: val_loss did not improve from 0.96624\n",
      "11/11 [==============================] - 7s 628ms/step - loss: 0.4774 - acc: 0.5714 - val_loss: 0.9870 - val_acc: 0.2426\n",
      "Epoch 5/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4387 - acc: 0.6121\n",
      "Epoch 00005: val_loss improved from 0.96624 to 0.95047, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 7s 669ms/step - loss: 0.4419 - acc: 0.6105 - val_loss: 0.9505 - val_acc: 0.5723\n",
      "Epoch 6/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4121 - acc: 0.6095\n",
      "Epoch 00006: val_loss did not improve from 0.95047\n",
      "11/11 [==============================] - 7s 641ms/step - loss: 0.3987 - acc: 0.6197 - val_loss: 0.9636 - val_acc: 0.5470\n",
      "Epoch 7/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3986 - acc: 0.6227\n",
      "Epoch 00007: val_loss did not improve from 0.95047\n",
      "11/11 [==============================] - 7s 640ms/step - loss: 0.3994 - acc: 0.6263 - val_loss: 1.0561 - val_acc: 0.5636\n",
      "Epoch 8/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4521 - acc: 0.6061\n",
      "Epoch 00008: val_loss did not improve from 0.95047\n",
      "11/11 [==============================] - 7s 630ms/step - loss: 0.4569 - acc: 0.6069 - val_loss: 1.1066 - val_acc: 0.3082\n",
      "Epoch 9/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.4206 - acc: 0.6311\n",
      "Epoch 00009: val_loss improved from 0.95047 to 0.90769, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 7s 673ms/step - loss: 0.4205 - acc: 0.6344 - val_loss: 0.9077 - val_acc: 0.5130\n",
      "Epoch 10/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3685 - acc: 0.6672\n",
      "Epoch 00010: val_loss improved from 0.90769 to 0.87588, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 7s 653ms/step - loss: 0.3897 - acc: 0.6541 - val_loss: 0.8759 - val_acc: 0.5567\n",
      "Epoch 11/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3715 - acc: 0.6496\n",
      "Epoch 00011: val_loss did not improve from 0.87588\n",
      "11/11 [==============================] - 7s 626ms/step - loss: 0.3793 - acc: 0.6416 - val_loss: 0.9106 - val_acc: 0.5608\n",
      "Epoch 12/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3877 - acc: 0.6272\n",
      "Epoch 00012: val_loss did not improve from 0.87588\n",
      "11/11 [==============================] - 7s 645ms/step - loss: 0.3897 - acc: 0.6310 - val_loss: 0.9477 - val_acc: 0.5485\n",
      "Epoch 13/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3812 - acc: 0.6509\n",
      "Epoch 00013: val_loss did not improve from 0.87588\n",
      "11/11 [==============================] - 7s 648ms/step - loss: 0.3732 - acc: 0.6611 - val_loss: 0.9592 - val_acc: 0.5980\n",
      "Epoch 14/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3652 - acc: 0.6471\n",
      "Epoch 00014: val_loss did not improve from 0.87588\n",
      "11/11 [==============================] - 7s 644ms/step - loss: 0.3659 - acc: 0.6454 - val_loss: 0.9214 - val_acc: 0.5400\n",
      "Epoch 15/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3477 - acc: 0.6838\n",
      "Epoch 00015: val_loss did not improve from 0.87588\n",
      "11/11 [==============================] - 7s 640ms/step - loss: 0.3497 - acc: 0.6865 - val_loss: 0.9995 - val_acc: 0.4843\n",
      "Epoch 16/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3456 - acc: 0.6627\n",
      "Epoch 00016: val_loss did not improve from 0.87588\n",
      "11/11 [==============================] - 7s 646ms/step - loss: 0.3464 - acc: 0.6576 - val_loss: 0.9143 - val_acc: 0.5441\n",
      "Epoch 17/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3376 - acc: 0.6962\n",
      "Epoch 00017: val_loss did not improve from 0.87588\n",
      "11/11 [==============================] - 7s 650ms/step - loss: 0.3397 - acc: 0.6905 - val_loss: 0.9614 - val_acc: 0.5698\n",
      "Epoch 18/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3555 - acc: 0.6829\n",
      "Epoch 00018: val_loss did not improve from 0.87588\n",
      "11/11 [==============================] - 7s 647ms/step - loss: 0.3608 - acc: 0.6824 - val_loss: 1.0677 - val_acc: 0.4597\n",
      "Epoch 19/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3531 - acc: 0.6707\n",
      "Epoch 00019: val_loss did not improve from 0.87588\n",
      "11/11 [==============================] - 7s 661ms/step - loss: 0.3466 - acc: 0.6797 - val_loss: 0.8799 - val_acc: 0.5739\n",
      "Epoch 20/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 0.3177 - acc: 0.7141\n",
      "Epoch 00020: val_loss did not improve from 0.87588\n",
      "11/11 [==============================] - 7s 659ms/step - loss: 0.3202 - acc: 0.7065 - val_loss: 0.9417 - val_acc: 0.5627\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (3842,)), ('top', (3842,)), ('base', (3842,)), ('pseudoGR', (3842, 32, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1917,)), ('top', (1917,)), ('base', (1917,)), ('pseudoGR', (1917, 32, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (13006,)), ('top', (13006,)), ('base', (13006,)), ('pseudoGR', (13006, 32, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1884,)), ('top', (1884,)), ('base', (1884,)), ('pseudoGR', (1884, 32, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (3873,)), ('top', (3873,)), ('base', (3873,)), ('pseudoGR', (3873, 32, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 128, 4)\n",
      "Epoch 1/20\n",
      "19/20 [===========================>..] - ETA: 1s - loss: 2.4243 - acc: 0.2287\n",
      "Epoch 00001: val_loss improved from inf to 0.85002, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 53s 3s/step - loss: 2.4328 - acc: 0.2267 - val_loss: 0.8500 - val_acc: 0.4245\n",
      "Epoch 2/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.9930 - acc: 0.3359\n",
      "Epoch 00002: val_loss improved from 0.85002 to 0.51091, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 8s 422ms/step - loss: 0.9736 - acc: 0.3510 - val_loss: 0.5109 - val_acc: 0.4216\n",
      "Epoch 3/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.7716 - acc: 0.5168\n",
      "Epoch 00003: val_loss did not improve from 0.51091\n",
      "20/20 [==============================] - 8s 406ms/step - loss: 0.7670 - acc: 0.5075 - val_loss: 0.5328 - val_acc: 0.3980\n",
      "Epoch 4/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.7183 - acc: 0.4977\n",
      "Epoch 00004: val_loss improved from 0.51091 to 0.47451, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 8s 421ms/step - loss: 0.7080 - acc: 0.5092 - val_loss: 0.4745 - val_acc: 0.4484\n",
      "Epoch 5/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6903 - acc: 0.5678\n",
      "Epoch 00005: val_loss improved from 0.47451 to 0.41111, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 8s 423ms/step - loss: 0.6787 - acc: 0.5685 - val_loss: 0.4111 - val_acc: 0.4239\n",
      "Epoch 6/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6439 - acc: 0.5648\n",
      "Epoch 00006: val_loss did not improve from 0.41111\n",
      "20/20 [==============================] - 8s 404ms/step - loss: 0.6344 - acc: 0.5631 - val_loss: 0.4436 - val_acc: 0.4399\n",
      "Epoch 7/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6726 - acc: 0.5712\n",
      "Epoch 00007: val_loss did not improve from 0.41111\n",
      "20/20 [==============================] - 8s 405ms/step - loss: 0.6730 - acc: 0.5706 - val_loss: 0.4601 - val_acc: 0.4498\n",
      "Epoch 8/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6259 - acc: 0.6245\n",
      "Epoch 00008: val_loss improved from 0.41111 to 0.39221, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 9s 426ms/step - loss: 0.6257 - acc: 0.6217 - val_loss: 0.3922 - val_acc: 0.4605\n",
      "Epoch 9/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6178 - acc: 0.6179\n",
      "Epoch 00009: val_loss improved from 0.39221 to 0.38070, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 8s 422ms/step - loss: 0.6239 - acc: 0.6197 - val_loss: 0.3807 - val_acc: 0.4579\n",
      "Epoch 10/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6014 - acc: 0.6271\n",
      "Epoch 00010: val_loss did not improve from 0.38070\n",
      "20/20 [==============================] - 8s 407ms/step - loss: 0.5877 - acc: 0.6293 - val_loss: 0.4010 - val_acc: 0.4828\n",
      "Epoch 11/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5703 - acc: 0.6486\n",
      "Epoch 00011: val_loss did not improve from 0.38070\n",
      "20/20 [==============================] - 8s 406ms/step - loss: 0.5640 - acc: 0.6481 - val_loss: 0.4079 - val_acc: 0.4646\n",
      "Epoch 12/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5625 - acc: 0.6507\n",
      "Epoch 00012: val_loss did not improve from 0.38070\n",
      "20/20 [==============================] - 8s 413ms/step - loss: 0.5736 - acc: 0.6417 - val_loss: 0.5034 - val_acc: 0.5029\n",
      "Epoch 13/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5756 - acc: 0.6486\n",
      "Epoch 00013: val_loss did not improve from 0.38070\n",
      "20/20 [==============================] - 8s 421ms/step - loss: 0.5651 - acc: 0.6556 - val_loss: 0.3878 - val_acc: 0.5314\n",
      "Epoch 14/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6080 - acc: 0.6027\n",
      "Epoch 00014: val_loss did not improve from 0.38070\n",
      "20/20 [==============================] - 8s 416ms/step - loss: 0.6066 - acc: 0.6079 - val_loss: 0.4636 - val_acc: 0.4155\n",
      "Epoch 15/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.6083 - acc: 0.6124\n",
      "Epoch 00015: val_loss did not improve from 0.38070\n",
      "20/20 [==============================] - 8s 418ms/step - loss: 0.6246 - acc: 0.6083 - val_loss: 0.4442 - val_acc: 0.4913\n",
      "Epoch 16/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5804 - acc: 0.6438\n",
      "Epoch 00016: val_loss did not improve from 0.38070\n",
      "20/20 [==============================] - 9s 426ms/step - loss: 0.5766 - acc: 0.6492 - val_loss: 0.3975 - val_acc: 0.4932\n",
      "Epoch 17/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5638 - acc: 0.6721\n",
      "Epoch 00017: val_loss did not improve from 0.38070\n",
      "20/20 [==============================] - 8s 419ms/step - loss: 0.5550 - acc: 0.6722 - val_loss: 0.3972 - val_acc: 0.5653\n",
      "Epoch 18/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5391 - acc: 0.6649\n",
      "Epoch 00018: val_loss did not improve from 0.38070\n",
      "20/20 [==============================] - 8s 424ms/step - loss: 0.5260 - acc: 0.6705 - val_loss: 0.4427 - val_acc: 0.4464\n",
      "Epoch 19/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5262 - acc: 0.6814\n",
      "Epoch 00019: val_loss did not improve from 0.38070\n",
      "20/20 [==============================] - 8s 424ms/step - loss: 0.5391 - acc: 0.6729 - val_loss: 0.4420 - val_acc: 0.5389\n",
      "Epoch 20/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.5672 - acc: 0.6438\n",
      "Epoch 00020: val_loss did not improve from 0.38070\n",
      "20/20 [==============================] - 8s 424ms/step - loss: 0.5677 - acc: 0.6474 - val_loss: 0.4382 - val_acc: 0.4883\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1920,)), ('top', (1920,)), ('base', (1920,)), ('pseudoGR', (1920, 64, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1936,)), ('top', (1936,)), ('base', (1936,)), ('pseudoGR', (1936, 64, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (957,)), ('top', (957,)), ('base', (957,)), ('pseudoGR', (957, 64, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (6500,)), ('top', (6500,)), ('base', (6500,)), ('pseudoGR', (6500, 64, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (944,)), ('top', (944,)), ('base', (944,)), ('pseudoGR', (944, 64, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 64, 4)\n",
      "Epoch 1/20\n",
      "21/22 [===========================>..] - ETA: 1s - loss: 1.8208 - acc: 0.2335\n",
      "Epoch 00001: val_loss improved from inf to 3.07684, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 58s 3s/step - loss: 1.9950 - acc: 0.2234 - val_loss: 3.0768 - val_acc: 0.1406\n",
      "Epoch 2/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.9209 - acc: 0.4178\n",
      "Epoch 00002: val_loss improved from 3.07684 to 0.66642, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 393ms/step - loss: 0.9181 - acc: 0.4112 - val_loss: 0.6664 - val_acc: 0.5257\n",
      "Epoch 3/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6991 - acc: 0.4579\n",
      "Epoch 00003: val_loss improved from 0.66642 to 0.58930, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 394ms/step - loss: 0.7026 - acc: 0.4627 - val_loss: 0.5893 - val_acc: 0.6217\n",
      "Epoch 4/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6588 - acc: 0.5445\n",
      "Epoch 00004: val_loss did not improve from 0.58930\n",
      "22/22 [==============================] - 8s 379ms/step - loss: 0.6528 - acc: 0.5391 - val_loss: 0.6148 - val_acc: 0.5898\n",
      "Epoch 5/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6046 - acc: 0.5471\n",
      "Epoch 00005: val_loss improved from 0.58930 to 0.58344, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 400ms/step - loss: 0.6221 - acc: 0.5426 - val_loss: 0.5834 - val_acc: 0.5977\n",
      "Epoch 6/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6362 - acc: 0.5662\n",
      "Epoch 00006: val_loss did not improve from 0.58344\n",
      "22/22 [==============================] - 8s 378ms/step - loss: 0.6280 - acc: 0.5663 - val_loss: 0.7525 - val_acc: 0.5251\n",
      "Epoch 7/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6225 - acc: 0.5673\n",
      "Epoch 00007: val_loss improved from 0.58344 to 0.58257, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.6250 - acc: 0.5652 - val_loss: 0.5826 - val_acc: 0.5938\n",
      "Epoch 8/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5889 - acc: 0.5997\n",
      "Epoch 00008: val_loss did not improve from 0.58257\n",
      "22/22 [==============================] - 8s 379ms/step - loss: 0.6005 - acc: 0.5937 - val_loss: 0.5858 - val_acc: 0.6133\n",
      "Epoch 9/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5763 - acc: 0.6125\n",
      "Epoch 00009: val_loss did not improve from 0.58257\n",
      "22/22 [==============================] - 8s 382ms/step - loss: 0.5849 - acc: 0.6106 - val_loss: 0.6818 - val_acc: 0.5681\n",
      "Epoch 10/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5759 - acc: 0.6152\n",
      "Epoch 00010: val_loss improved from 0.58257 to 0.57682, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 395ms/step - loss: 0.5803 - acc: 0.6148 - val_loss: 0.5768 - val_acc: 0.5904\n",
      "Epoch 11/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5770 - acc: 0.6154\n",
      "Epoch 00011: val_loss did not improve from 0.57682\n",
      "22/22 [==============================] - 8s 385ms/step - loss: 0.5659 - acc: 0.6193 - val_loss: 0.6404 - val_acc: 0.5910\n",
      "Epoch 12/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6052 - acc: 0.5758\n",
      "Epoch 00012: val_loss did not improve from 0.57682\n",
      "22/22 [==============================] - 9s 389ms/step - loss: 0.5964 - acc: 0.5803 - val_loss: 0.6135 - val_acc: 0.5742\n",
      "Epoch 13/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5726 - acc: 0.5999\n",
      "Epoch 00013: val_loss improved from 0.57682 to 0.55082, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 412ms/step - loss: 0.5710 - acc: 0.6027 - val_loss: 0.5508 - val_acc: 0.6172\n",
      "Epoch 14/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5441 - acc: 0.6213\n",
      "Epoch 00014: val_loss did not improve from 0.55082\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.5475 - acc: 0.6211 - val_loss: 0.5614 - val_acc: 0.6295\n",
      "Epoch 15/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5848 - acc: 0.5979\n",
      "Epoch 00015: val_loss did not improve from 0.55082\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.5748 - acc: 0.6046 - val_loss: 0.5982 - val_acc: 0.6055\n",
      "Epoch 16/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5786 - acc: 0.6135\n",
      "Epoch 00016: val_loss did not improve from 0.55082\n",
      "22/22 [==============================] - 9s 395ms/step - loss: 0.5680 - acc: 0.6163 - val_loss: 0.5900 - val_acc: 0.6345\n",
      "Epoch 17/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5422 - acc: 0.6336\n",
      "Epoch 00017: val_loss did not improve from 0.55082\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 0.5299 - acc: 0.6372 - val_loss: 0.6152 - val_acc: 0.6055\n",
      "Epoch 18/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5412 - acc: 0.6396\n",
      "Epoch 00018: val_loss did not improve from 0.55082\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.5373 - acc: 0.6448 - val_loss: 0.7060 - val_acc: 0.5725\n",
      "Epoch 19/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5217 - acc: 0.6598\n",
      "Epoch 00019: val_loss did not improve from 0.55082\n",
      "22/22 [==============================] - 9s 394ms/step - loss: 0.5130 - acc: 0.6636 - val_loss: 0.6125 - val_acc: 0.6138\n",
      "Epoch 20/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5063 - acc: 0.6734\n",
      "Epoch 00020: val_loss did not improve from 0.55082\n",
      "22/22 [==============================] - 9s 395ms/step - loss: 0.5004 - acc: 0.6747 - val_loss: 0.6125 - val_acc: 0.6205\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1920,)), ('top', (1920,)), ('base', (1920,)), ('pseudoGR', (1920, 64, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1936,)), ('top', (1936,)), ('base', (1936,)), ('pseudoGR', (1936, 64, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (957,)), ('top', (957,)), ('base', (957,)), ('pseudoGR', (957, 64, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (944,)), ('top', (944,)), ('base', (944,)), ('pseudoGR', (944, 64, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (6500,)), ('top', (6500,)), ('base', (6500,)), ('pseudoGR', (6500, 64, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 64, 4)\n",
      "Epoch 1/20\n",
      "10/11 [==========================>...] - ETA: 2s - loss: 2.7179 - acc: 0.3290\n",
      "Epoch 00001: val_loss improved from inf to 3.69000, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 65s 6s/step - loss: 2.8339 - acc: 0.3112 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 2/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9183 - acc: 0.3279\n",
      "Epoch 00002: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 673ms/step - loss: 2.8726 - acc: 0.3344 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 3/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8795 - acc: 0.3333\n",
      "Epoch 00003: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 676ms/step - loss: 2.8833 - acc: 0.3330 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 4/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8630 - acc: 0.3363\n",
      "Epoch 00004: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 675ms/step - loss: 2.8879 - acc: 0.3321 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 5/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9040 - acc: 0.3304\n",
      "Epoch 00005: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 673ms/step - loss: 2.8765 - acc: 0.3338 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 6/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8645 - acc: 0.3389\n",
      "Epoch 00006: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 668ms/step - loss: 2.8875 - acc: 0.3314 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 7/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9656 - acc: 0.3139\n",
      "Epoch 00007: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 8s 682ms/step - loss: 2.8594 - acc: 0.3384 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 8/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8934 - acc: 0.3289\n",
      "Epoch 00008: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 659ms/step - loss: 2.8795 - acc: 0.3342 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 9/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8948 - acc: 0.3295\n",
      "Epoch 00009: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 651ms/step - loss: 2.8791 - acc: 0.3340 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 10/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8383 - acc: 0.3430\n",
      "Epoch 00010: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 660ms/step - loss: 2.8948 - acc: 0.3303 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 11/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9451 - acc: 0.3159\n",
      "Epoch 00011: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 662ms/step - loss: 2.8651 - acc: 0.3378 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 12/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8652 - acc: 0.3378\n",
      "Epoch 00012: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 682ms/step - loss: 2.8873 - acc: 0.3317 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 13/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9034 - acc: 0.3277\n",
      "Epoch 00013: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 672ms/step - loss: 2.8767 - acc: 0.3345 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 14/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8663 - acc: 0.3377\n",
      "Epoch 00014: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 668ms/step - loss: 2.8870 - acc: 0.3317 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 15/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9356 - acc: 0.3236\n",
      "Epoch 00015: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 679ms/step - loss: 2.8677 - acc: 0.3356 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 16/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8556 - acc: 0.3398\n",
      "Epoch 00016: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 671ms/step - loss: 2.8900 - acc: 0.3311 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 17/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8350 - acc: 0.3455\n",
      "Epoch 00017: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 660ms/step - loss: 2.8939 - acc: 0.3297 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 18/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8678 - acc: 0.3358\n",
      "Epoch 00018: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 678ms/step - loss: 2.8796 - acc: 0.3333 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 19/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8821 - acc: 0.3302\n",
      "Epoch 00019: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 8s 687ms/step - loss: 2.8925 - acc: 0.3267 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Epoch 20/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9481 - acc: 0.3216\n",
      "Epoch 00020: val_loss did not improve from 3.69000\n",
      "11/11 [==============================] - 7s 676ms/step - loss: 2.8644 - acc: 0.3361 - val_loss: 3.6900 - val_acc: 0.1591\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1920,)), ('top', (1920,)), ('base', (1920,)), ('pseudoGR', (1920, 64, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (957,)), ('top', (957,)), ('base', (957,)), ('pseudoGR', (957, 64, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (6500,)), ('top', (6500,)), ('base', (6500,)), ('pseudoGR', (6500, 64, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (944,)), ('top', (944,)), ('base', (944,)), ('pseudoGR', (944, 64, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1936,)), ('top', (1936,)), ('base', (1936,)), ('pseudoGR', (1936, 64, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 64, 4)\n",
      "Epoch 1/20\n",
      "19/20 [===========================>..] - ETA: 1s - loss: 3.3410 - acc: 0.1812\n",
      "Epoch 00001: val_loss improved from inf to 3.76693, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 70s 3s/step - loss: 3.3463 - acc: 0.1770 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 2/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4741 - acc: 0.1737\n",
      "Epoch 00002: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 410ms/step - loss: 3.4760 - acc: 0.1748 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 3/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4941 - acc: 0.1702\n",
      "Epoch 00003: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 408ms/step - loss: 3.4678 - acc: 0.1763 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 4/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4659 - acc: 0.1781\n",
      "Epoch 00004: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 412ms/step - loss: 3.4794 - acc: 0.1730 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 5/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4758 - acc: 0.1758\n",
      "Epoch 00005: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 412ms/step - loss: 3.4753 - acc: 0.1740 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 6/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4778 - acc: 0.1718\n",
      "Epoch 00006: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 411ms/step - loss: 3.4745 - acc: 0.1756 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 7/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4778 - acc: 0.1765\n",
      "Epoch 00007: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 409ms/step - loss: 3.4745 - acc: 0.1737 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 8/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4805 - acc: 0.1737\n",
      "Epoch 00008: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 412ms/step - loss: 3.4735 - acc: 0.1748 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 9/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4714 - acc: 0.1782\n",
      "Epoch 00009: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 411ms/step - loss: 3.4771 - acc: 0.1730 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 10/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4763 - acc: 0.1696\n",
      "Epoch 00010: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 412ms/step - loss: 3.4751 - acc: 0.1765 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 11/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4816 - acc: 0.1728\n",
      "Epoch 00011: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 414ms/step - loss: 3.4729 - acc: 0.1752 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 12/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4551 - acc: 0.1781\n",
      "Epoch 00012: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 413ms/step - loss: 3.4838 - acc: 0.1730 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 13/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4505 - acc: 0.1785\n",
      "Epoch 00013: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 418ms/step - loss: 3.4855 - acc: 0.1729 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 14/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4963 - acc: 0.1799\n",
      "Epoch 00014: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 423ms/step - loss: 3.4605 - acc: 0.1829 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 15/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4606 - acc: 0.1769\n",
      "Epoch 00015: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 9s 426ms/step - loss: 3.4809 - acc: 0.1734 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 16/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4634 - acc: 0.1762\n",
      "Epoch 00016: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 9s 430ms/step - loss: 3.4807 - acc: 0.1730 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 17/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4804 - acc: 0.1762\n",
      "Epoch 00017: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 9s 433ms/step - loss: 3.4734 - acc: 0.1738 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 18/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4731 - acc: 0.1759\n",
      "Epoch 00018: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 9s 431ms/step - loss: 3.4764 - acc: 0.1739 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 19/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4869 - acc: 0.1733\n",
      "Epoch 00019: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 8s 425ms/step - loss: 3.4707 - acc: 0.1750 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Epoch 20/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4842 - acc: 0.1752\n",
      "Epoch 00020: val_loss did not improve from 3.76693\n",
      "20/20 [==============================] - 9s 428ms/step - loss: 3.4718 - acc: 0.1742 - val_loss: 3.7669 - val_acc: 0.1374\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (960,)), ('top', (960,)), ('base', (960,)), ('pseudoGR', (960, 128, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (965,)), ('top', (965,)), ('base', (965,)), ('pseudoGR', (965, 128, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (481,)), ('top', (481,)), ('base', (481,)), ('pseudoGR', (481, 128, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (3253,)), ('top', (3253,)), ('base', (3253,)), ('pseudoGR', (3253, 128, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (473,)), ('top', (473,)), ('base', (473,)), ('pseudoGR', (473, 128, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 32, 4)\n",
      "Epoch 1/20\n",
      "21/22 [===========================>..] - ETA: 1s - loss: 1.8155 - acc: 0.4036\n",
      "Epoch 00001: val_loss improved from inf to 3.30618, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 78s 4s/step - loss: 2.0219 - acc: 0.3864 - val_loss: 3.3062 - val_acc: 0.1562\n",
      "Epoch 2/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 1.7117 - acc: 0.3231\n",
      "Epoch 00002: val_loss improved from 3.30618 to 1.06501, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 1.6918 - acc: 0.3316 - val_loss: 1.0650 - val_acc: 0.5458\n",
      "Epoch 3/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.9072 - acc: 0.4486\n",
      "Epoch 00003: val_loss improved from 1.06501 to 0.63097, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.8916 - acc: 0.4468 - val_loss: 0.6310 - val_acc: 0.5971\n",
      "Epoch 4/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7093 - acc: 0.4971\n",
      "Epoch 00004: val_loss improved from 0.63097 to 0.60557, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 399ms/step - loss: 0.6956 - acc: 0.5027 - val_loss: 0.6056 - val_acc: 0.6004\n",
      "Epoch 5/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6440 - acc: 0.5248\n",
      "Epoch 00005: val_loss improved from 0.60557 to 0.57926, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 399ms/step - loss: 0.6535 - acc: 0.5169 - val_loss: 0.5793 - val_acc: 0.6094\n",
      "Epoch 6/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6374 - acc: 0.5539\n",
      "Epoch 00006: val_loss did not improve from 0.57926\n",
      "22/22 [==============================] - 8s 376ms/step - loss: 0.6329 - acc: 0.5473 - val_loss: 0.7418 - val_acc: 0.5547\n",
      "Epoch 7/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6292 - acc: 0.5288\n",
      "Epoch 00007: val_loss did not improve from 0.57926\n",
      "22/22 [==============================] - 8s 377ms/step - loss: 0.6216 - acc: 0.5323 - val_loss: 0.6140 - val_acc: 0.5379\n",
      "Epoch 8/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6184 - acc: 0.5615\n",
      "Epoch 00008: val_loss did not improve from 0.57926\n",
      "22/22 [==============================] - 8s 378ms/step - loss: 0.6084 - acc: 0.5758 - val_loss: 0.6042 - val_acc: 0.5993\n",
      "Epoch 9/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5789 - acc: 0.5991\n",
      "Epoch 00009: val_loss improved from 0.57926 to 0.55657, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 9s 399ms/step - loss: 0.5809 - acc: 0.6031 - val_loss: 0.5566 - val_acc: 0.6060\n",
      "Epoch 10/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5681 - acc: 0.6211\n",
      "Epoch 00010: val_loss did not improve from 0.55657\n",
      "22/22 [==============================] - 8s 380ms/step - loss: 0.5546 - acc: 0.6238 - val_loss: 0.5626 - val_acc: 0.5993\n",
      "Epoch 11/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5529 - acc: 0.6228\n",
      "Epoch 00011: val_loss did not improve from 0.55657\n",
      "22/22 [==============================] - 9s 389ms/step - loss: 0.5478 - acc: 0.6252 - val_loss: 0.6494 - val_acc: 0.6183\n",
      "Epoch 12/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5507 - acc: 0.5991\n",
      "Epoch 00012: val_loss did not improve from 0.55657\n",
      "22/22 [==============================] - 9s 391ms/step - loss: 0.5534 - acc: 0.5941 - val_loss: 0.6092 - val_acc: 0.6161\n",
      "Epoch 13/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5483 - acc: 0.6363\n",
      "Epoch 00013: val_loss did not improve from 0.55657\n",
      "22/22 [==============================] - 9s 395ms/step - loss: 0.5467 - acc: 0.6375 - val_loss: 0.5945 - val_acc: 0.5625\n",
      "Epoch 14/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5703 - acc: 0.6215\n",
      "Epoch 00014: val_loss did not improve from 0.55657\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 0.5648 - acc: 0.6225 - val_loss: 0.6640 - val_acc: 0.5804\n",
      "Epoch 15/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5554 - acc: 0.6260\n",
      "Epoch 00015: val_loss did not improve from 0.55657\n",
      "22/22 [==============================] - 9s 396ms/step - loss: 0.5684 - acc: 0.6261 - val_loss: 0.6218 - val_acc: 0.6183\n",
      "Epoch 16/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6023 - acc: 0.5889\n",
      "Epoch 00016: val_loss did not improve from 0.55657\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 0.5963 - acc: 0.5936 - val_loss: 0.6092 - val_acc: 0.5848\n",
      "Epoch 17/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5674 - acc: 0.6097\n",
      "Epoch 00017: val_loss did not improve from 0.55657\n",
      "22/22 [==============================] - 9s 398ms/step - loss: 0.5853 - acc: 0.6062 - val_loss: 0.7010 - val_acc: 0.5201\n",
      "Epoch 18/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5613 - acc: 0.6378\n",
      "Epoch 00018: val_loss did not improve from 0.55657\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 0.5664 - acc: 0.6340 - val_loss: 0.6463 - val_acc: 0.5413\n",
      "Epoch 19/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5202 - acc: 0.6445\n",
      "Epoch 00019: val_loss did not improve from 0.55657\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 0.5265 - acc: 0.6430 - val_loss: 0.5615 - val_acc: 0.6138\n",
      "Epoch 20/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5197 - acc: 0.6558\n",
      "Epoch 00020: val_loss did not improve from 0.55657\n",
      "22/22 [==============================] - 9s 397ms/step - loss: 0.5124 - acc: 0.6569 - val_loss: 0.6678 - val_acc: 0.6083\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (960,)), ('top', (960,)), ('base', (960,)), ('pseudoGR', (960, 128, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (965,)), ('top', (965,)), ('base', (965,)), ('pseudoGR', (965, 128, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (481,)), ('top', (481,)), ('base', (481,)), ('pseudoGR', (481, 128, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (473,)), ('top', (473,)), ('base', (473,)), ('pseudoGR', (473, 128, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (3253,)), ('top', (3253,)), ('base', (3253,)), ('pseudoGR', (3253, 128, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 32, 4)\n",
      "Epoch 1/20\n",
      "10/11 [==========================>...] - ETA: 2s - loss: 2.7535 - acc: 0.3113\n",
      "Epoch 00001: val_loss improved from inf to 3.68610, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 81s 7s/step - loss: 2.8643 - acc: 0.2952 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 2/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9318 - acc: 0.3225\n",
      "Epoch 00002: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 714ms/step - loss: 2.8678 - acc: 0.3355 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 3/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9267 - acc: 0.3260\n",
      "Epoch 00003: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 719ms/step - loss: 2.8692 - acc: 0.3345 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 4/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9113 - acc: 0.3262\n",
      "Epoch 00004: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 705ms/step - loss: 2.8734 - acc: 0.3345 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 5/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8362 - acc: 0.3443\n",
      "Epoch 00005: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 723ms/step - loss: 2.8943 - acc: 0.3294 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 6/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8110 - acc: 0.3477\n",
      "Epoch 00006: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 706ms/step - loss: 2.9013 - acc: 0.3285 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 7/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8422 - acc: 0.3416\n",
      "Epoch 00007: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 721ms/step - loss: 2.8926 - acc: 0.3302 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 8/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8938 - acc: 0.3311\n",
      "Epoch 00008: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 719ms/step - loss: 2.8783 - acc: 0.3331 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 9/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8743 - acc: 0.3340\n",
      "Epoch 00009: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 759ms/step - loss: 2.8837 - acc: 0.3323 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 10/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8531 - acc: 0.3369\n",
      "Epoch 00010: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 735ms/step - loss: 2.8896 - acc: 0.3315 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 11/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8917 - acc: 0.3293\n",
      "Epoch 00011: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 701ms/step - loss: 2.8789 - acc: 0.3336 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 12/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8964 - acc: 0.3281\n",
      "Epoch 00012: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 713ms/step - loss: 2.8776 - acc: 0.3339 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 13/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8568 - acc: 0.3400\n",
      "Epoch 00013: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 716ms/step - loss: 2.8886 - acc: 0.3306 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 14/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8557 - acc: 0.3385\n",
      "Epoch 00014: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 712ms/step - loss: 2.8889 - acc: 0.3311 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 15/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8579 - acc: 0.3377\n",
      "Epoch 00015: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 747ms/step - loss: 2.8883 - acc: 0.3313 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 16/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8813 - acc: 0.3318\n",
      "Epoch 00016: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 720ms/step - loss: 2.8818 - acc: 0.3329 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 17/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8958 - acc: 0.3283\n",
      "Epoch 00017: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 736ms/step - loss: 2.8777 - acc: 0.3339 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 18/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8447 - acc: 0.3404\n",
      "Epoch 00018: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 743ms/step - loss: 2.8920 - acc: 0.3305 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 19/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8208 - acc: 0.3459\n",
      "Epoch 00019: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 743ms/step - loss: 2.8986 - acc: 0.3290 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Epoch 20/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8590 - acc: 0.3381\n",
      "Epoch 00020: val_loss did not improve from 3.68610\n",
      "11/11 [==============================] - 8s 758ms/step - loss: 2.8880 - acc: 0.3312 - val_loss: 3.6861 - val_acc: 0.1597\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (960,)), ('top', (960,)), ('base', (960,)), ('pseudoGR', (960, 128, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (481,)), ('top', (481,)), ('base', (481,)), ('pseudoGR', (481, 128, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (3253,)), ('top', (3253,)), ('base', (3253,)), ('pseudoGR', (3253, 128, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (473,)), ('top', (473,)), ('base', (473,)), ('pseudoGR', (473, 128, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (965,)), ('top', (965,)), ('base', (965,)), ('pseudoGR', (965, 128, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 32, 4)\n",
      "Epoch 1/20\n",
      "19/20 [===========================>..] - ETA: 1s - loss: 3.5840 - acc: 0.1527\n",
      "Epoch 00001: val_loss improved from inf to 3.75864, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 88s 4s/step - loss: 3.5816 - acc: 0.1496 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 2/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.5088 - acc: 0.1721\n",
      "Epoch 00002: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 8s 421ms/step - loss: 3.4670 - acc: 0.1749 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 3/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4886 - acc: 0.1720\n",
      "Epoch 00003: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 444ms/step - loss: 3.4746 - acc: 0.1750 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 4/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4619 - acc: 0.1762\n",
      "Epoch 00004: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 444ms/step - loss: 3.4865 - acc: 0.1731 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 5/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4609 - acc: 0.1775\n",
      "Epoch 00005: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 445ms/step - loss: 3.4860 - acc: 0.1727 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 6/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4752 - acc: 0.1750\n",
      "Epoch 00006: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 446ms/step - loss: 3.4810 - acc: 0.1736 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 7/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4987 - acc: 0.1711\n",
      "Epoch 00007: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 425ms/step - loss: 3.4723 - acc: 0.1755 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 8/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.5230 - acc: 0.1741\n",
      "Epoch 00008: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 465ms/step - loss: 3.4912 - acc: 0.1772 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 9/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.5160 - acc: 0.1687\n",
      "Epoch 00009: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 425ms/step - loss: 3.4633 - acc: 0.1763 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 10/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4509 - acc: 0.1785\n",
      "Epoch 00010: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 425ms/step - loss: 3.4901 - acc: 0.1723 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 11/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4689 - acc: 0.1718\n",
      "Epoch 00011: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 8s 423ms/step - loss: 3.4827 - acc: 0.1751 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 12/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4687 - acc: 0.1736\n",
      "Epoch 00012: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 462ms/step - loss: 3.4828 - acc: 0.1743 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 13/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.5106 - acc: 0.1692\n",
      "Epoch 00013: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 430ms/step - loss: 3.4655 - acc: 0.1761 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 14/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4842 - acc: 0.1723\n",
      "Epoch 00014: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 439ms/step - loss: 3.4764 - acc: 0.1749 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 15/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4887 - acc: 0.1698\n",
      "Epoch 00015: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 440ms/step - loss: 3.4745 - acc: 0.1759 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 16/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4709 - acc: 0.1768\n",
      "Epoch 00016: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 442ms/step - loss: 3.4819 - acc: 0.1730 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 17/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4873 - acc: 0.1703\n",
      "Epoch 00017: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 445ms/step - loss: 3.4751 - acc: 0.1757 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 18/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.5043 - acc: 0.1685\n",
      "Epoch 00018: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 463ms/step - loss: 3.4681 - acc: 0.1764 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 19/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4924 - acc: 0.1749\n",
      "Epoch 00019: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 9s 452ms/step - loss: 3.4730 - acc: 0.1738 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Epoch 20/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.4891 - acc: 0.1693\n",
      "Epoch 00020: val_loss did not improve from 3.75864\n",
      "20/20 [==============================] - 10s 482ms/step - loss: 3.4744 - acc: 0.1761 - val_loss: 3.7586 - val_acc: 0.1393\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (477,)), ('top', (477,)), ('base', (477,)), ('pseudoGR', (477, 256, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (483,)), ('top', (483,)), ('base', (483,)), ('pseudoGR', (483, 256, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (240,)), ('top', (240,)), ('base', (240,)), ('pseudoGR', (240, 256, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1626,)), ('top', (1626,)), ('base', (1626,)), ('pseudoGR', (1626, 256, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (235,)), ('top', (235,)), ('base', (235,)), ('pseudoGR', (235, 256, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 16, 4)\n",
      "Epoch 1/20\n",
      "21/22 [===========================>..] - ETA: 1s - loss: 3.1436 - acc: 0.2145\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.75296, saving model to temp_best_weights.h5\n",
      "22/22 [==============================] - 98s 4s/step - loss: 3.1794 - acc: 0.2110 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 2/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.0942 - acc: 0.2385\n",
      "Epoch 00002: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 402ms/step - loss: 3.0684 - acc: 0.2437 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 3/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.0886 - acc: 0.2444\n",
      "Epoch 00003: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 394ms/step - loss: 3.1058 - acc: 0.2394 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 4/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1369 - acc: 0.2400\n",
      "Epoch 00004: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 403ms/step - loss: 3.1123 - acc: 0.2418 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 5/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1105 - acc: 0.2414\n",
      "Epoch 00005: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 3.1267 - acc: 0.2410 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 6/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1041 - acc: 0.2463\n",
      "Epoch 00006: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 8s 383ms/step - loss: 3.1303 - acc: 0.2383 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 7/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1165 - acc: 0.2422\n",
      "Epoch 00007: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 8s 382ms/step - loss: 3.1235 - acc: 0.2406 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 8/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1395 - acc: 0.2364\n",
      "Epoch 00008: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 395ms/step - loss: 3.1108 - acc: 0.2437 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 9/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1000 - acc: 0.2426\n",
      "Epoch 00009: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 389ms/step - loss: 3.1325 - acc: 0.2404 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 10/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.0984 - acc: 0.2448\n",
      "Epoch 00010: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 406ms/step - loss: 3.1334 - acc: 0.2392 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 11/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1253 - acc: 0.2394\n",
      "Epoch 00011: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 407ms/step - loss: 3.1186 - acc: 0.2421 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 12/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.0989 - acc: 0.2467\n",
      "Epoch 00012: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 387ms/step - loss: 3.1331 - acc: 0.2381 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 13/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1052 - acc: 0.2463\n",
      "Epoch 00013: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 412ms/step - loss: 3.1297 - acc: 0.2383 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 14/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1237 - acc: 0.2422\n",
      "Epoch 00014: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 420ms/step - loss: 3.1195 - acc: 0.2406 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 15/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1276 - acc: 0.2383\n",
      "Epoch 00015: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 399ms/step - loss: 3.1174 - acc: 0.2427 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 16/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1333 - acc: 0.2377\n",
      "Epoch 00016: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 411ms/step - loss: 3.1143 - acc: 0.2430 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 17/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1529 - acc: 0.2334\n",
      "Epoch 00017: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 400ms/step - loss: 3.1035 - acc: 0.2454 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 18/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1256 - acc: 0.2388\n",
      "Epoch 00018: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 404ms/step - loss: 3.1184 - acc: 0.2424 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 19/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1025 - acc: 0.2442\n",
      "Epoch 00019: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 405ms/step - loss: 3.1311 - acc: 0.2395 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Epoch 20/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 3.1238 - acc: 0.2398\n",
      "Epoch 00020: val_loss did not improve from 3.75296\n",
      "22/22 [==============================] - 9s 399ms/step - loss: 3.1194 - acc: 0.2419 - val_loss: 3.7530 - val_acc: 0.1562\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (477,)), ('top', (477,)), ('base', (477,)), ('pseudoGR', (477, 256, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (483,)), ('top', (483,)), ('base', (483,)), ('pseudoGR', (483, 256, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (240,)), ('top', (240,)), ('base', (240,)), ('pseudoGR', (240, 256, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (235,)), ('top', (235,)), ('base', (235,)), ('pseudoGR', (235, 256, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1626,)), ('top', (1626,)), ('base', (1626,)), ('pseudoGR', (1626, 256, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 16, 4)\n",
      "Epoch 1/20\n",
      "10/11 [==========================>...] - ETA: 2s - loss: 2.6987 - acc: 0.2941\n",
      "Epoch 00001: val_loss improved from inf to 3.69167, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 99s 9s/step - loss: 2.8191 - acc: 0.2786 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 2/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9350 - acc: 0.3219\n",
      "Epoch 00002: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 9s 774ms/step - loss: 2.9031 - acc: 0.3271 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 3/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9053 - acc: 0.3277\n",
      "Epoch 00003: val_loss improved from 3.69167 to 3.69167, saving model to temp_best_weights.h5\n",
      "11/11 [==============================] - 9s 854ms/step - loss: 2.9114 - acc: 0.3254 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 4/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8525 - acc: 0.3406\n",
      "Epoch 00004: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 8s 751ms/step - loss: 2.9260 - acc: 0.3219 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 5/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8567 - acc: 0.3383\n",
      "Epoch 00005: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 8s 752ms/step - loss: 2.9249 - acc: 0.3225 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 6/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9644 - acc: 0.3125\n",
      "Epoch 00006: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 8s 762ms/step - loss: 2.8950 - acc: 0.3297 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 7/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8741 - acc: 0.3348\n",
      "Epoch 00007: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 9s 799ms/step - loss: 2.9201 - acc: 0.3235 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 8/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9055 - acc: 0.3262\n",
      "Epoch 00008: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 9s 811ms/step - loss: 2.9113 - acc: 0.3259 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 9/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8711 - acc: 0.3379\n",
      "Epoch 00009: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 9s 788ms/step - loss: 2.9209 - acc: 0.3226 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 10/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9209 - acc: 0.3262\n",
      "Epoch 00010: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 9s 806ms/step - loss: 2.9071 - acc: 0.3259 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 11/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8849 - acc: 0.3289\n",
      "Epoch 00011: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 8s 757ms/step - loss: 2.9171 - acc: 0.3251 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 12/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9214 - acc: 0.3199\n",
      "Epoch 00012: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 8s 765ms/step - loss: 2.9069 - acc: 0.3276 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 13/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9194 - acc: 0.3246\n",
      "Epoch 00013: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 8s 757ms/step - loss: 2.9075 - acc: 0.3263 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 14/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8916 - acc: 0.3297\n",
      "Epoch 00014: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 9s 779ms/step - loss: 2.9152 - acc: 0.3249 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 15/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9378 - acc: 0.3187\n",
      "Epoch 00015: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 8s 748ms/step - loss: 2.9024 - acc: 0.3279 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 16/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8873 - acc: 0.3309\n",
      "Epoch 00016: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 9s 833ms/step - loss: 2.9164 - acc: 0.3246 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 17/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.9308 - acc: 0.3215\n",
      "Epoch 00017: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 9s 810ms/step - loss: 2.9043 - acc: 0.3272 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 18/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8874 - acc: 0.3309\n",
      "Epoch 00018: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 9s 833ms/step - loss: 2.9164 - acc: 0.3246 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 19/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8943 - acc: 0.3289\n",
      "Epoch 00019: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 9s 784ms/step - loss: 2.9144 - acc: 0.3251 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Epoch 20/20\n",
      "10/11 [==========================>...] - ETA: 0s - loss: 2.8778 - acc: 0.3332\n",
      "Epoch 00020: val_loss did not improve from 3.69167\n",
      "11/11 [==============================] - 8s 763ms/step - loss: 2.9190 - acc: 0.3239 - val_loss: 3.6917 - val_acc: 0.1572\n",
      "Loading Well:  205-21b-3  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (477,)), ('top', (477,)), ('base', (477,)), ('pseudoGR', (477, 256, 16))]\n",
      "Loading Well:  204-20-1Z  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (240,)), ('top', (240,)), ('base', (240,)), ('pseudoGR', (240, 256, 16))]\n",
      "Loading Well:  204-24a-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (1626,)), ('top', (1626,)), ('base', (1626,)), ('pseudoGR', (1626, 256, 16))]\n",
      "Loading Well:  204-19-6  from  /home/administrator/Dropbox/core_data/facies/train_data\n",
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (235,)), ('top', (235,)), ('base', (235,)), ('pseudoGR', (235, 256, 16))]\n",
      "Loading Well:  204-20-6a  from  /home/administrator/Dropbox/core_data/facies/train_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:86: RuntimeWarning: Mean of empty slice\n",
      "  output_features.append(np.nanmean(img, axis=1))\n",
      "/home/administrator/code/python/coremdlr/coremdlr/facies/datasets/utils.py:90: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  output_features.append(np.nanvar(img, axis=1))\n",
      "/home/administrator/anaconda3/envs/core-dev/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted pGR features:  ['Umean', 'Rmean', 'Gmean', 'Bmean', 'Uvar', 'Rvar', 'Gvar', 'Bvar', 'Up10', 'Rp10', 'Gp10', 'Bp10', 'Up90', 'Rp90', 'Gp90', 'Bp90']\n",
      "Feature shapes:  [('depth', (483,)), ('top', (483,)), ('base', (483,)), ('pseudoGR', (483, 256, 16))]\n",
      "Shapes of `(batch_X, batch_y)`: (16, 4096, 16), (16, 16, 4)\n",
      "Epoch 1/20\n",
      "19/20 [===========================>..] - ETA: 1s - loss: 3.1942 - acc: 0.1861\n",
      "Epoch 00001: val_loss improved from inf to 2.28245, saving model to temp_best_weights.h5\n",
      "20/20 [==============================] - 106s 5s/step - loss: 3.2255 - acc: 0.1825 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 2/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2565 - acc: 0.1953\n",
      "Epoch 00002: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 466ms/step - loss: 3.2218 - acc: 0.2050 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 3/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2274 - acc: 0.2025\n",
      "Epoch 00003: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 454ms/step - loss: 3.2304 - acc: 0.2023 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 4/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2456 - acc: 0.1988\n",
      "Epoch 00004: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 10s 476ms/step - loss: 3.2221 - acc: 0.2039 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 5/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2359 - acc: 0.1992\n",
      "Epoch 00005: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 441ms/step - loss: 3.2269 - acc: 0.2037 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 6/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2173 - acc: 0.2062\n",
      "Epoch 00006: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 453ms/step - loss: 3.2346 - acc: 0.2008 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 7/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2439 - acc: 0.2000\n",
      "Epoch 00007: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 472ms/step - loss: 3.2236 - acc: 0.2033 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 8/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2126 - acc: 0.2079\n",
      "Epoch 00008: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 443ms/step - loss: 3.2365 - acc: 0.2001 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 9/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2491 - acc: 0.1980\n",
      "Epoch 00009: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 469ms/step - loss: 3.2215 - acc: 0.2042 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 10/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2145 - acc: 0.2052\n",
      "Epoch 00010: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 456ms/step - loss: 3.2357 - acc: 0.2012 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 11/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2088 - acc: 0.2072\n",
      "Epoch 00011: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 10s 477ms/step - loss: 3.2381 - acc: 0.2004 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 12/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2400 - acc: 0.1984\n",
      "Epoch 00012: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 436ms/step - loss: 3.2252 - acc: 0.2040 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 13/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2426 - acc: 0.1986\n",
      "Epoch 00013: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 10s 476ms/step - loss: 3.2242 - acc: 0.2039 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 14/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2220 - acc: 0.2044\n",
      "Epoch 00014: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 10s 478ms/step - loss: 3.2326 - acc: 0.2016 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 15/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2190 - acc: 0.2031\n",
      "Epoch 00015: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 471ms/step - loss: 3.2339 - acc: 0.2021 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 16/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2164 - acc: 0.2068\n",
      "Epoch 00016: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 10s 490ms/step - loss: 3.2349 - acc: 0.2006 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 17/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2399 - acc: 0.2009\n",
      "Epoch 00017: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 10s 486ms/step - loss: 3.2253 - acc: 0.2030 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 18/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2342 - acc: 0.2009\n",
      "Epoch 00018: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 448ms/step - loss: 3.2276 - acc: 0.2030 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 19/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2211 - acc: 0.2042\n",
      "Epoch 00019: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 448ms/step - loss: 3.2330 - acc: 0.2017 - val_loss: 2.2824 - val_acc: 0.4142\n",
      "Epoch 20/20\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 3.2402 - acc: 0.1988\n",
      "Epoch 00020: val_loss did not improve from 2.28245\n",
      "20/20 [==============================] - 9s 472ms/step - loss: 3.2252 - acc: 0.2039 - val_loss: 2.2824 - val_acc: 0.4142\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for resolution in [8, 32, 64, 128, 256]:\n",
    "\n",
    "    for test_well in test_well_names:\n",
    "    \n",
    "        # Set up dataset\n",
    "        train_wells = [w for w in well_names if w != test_well]\n",
    "        \n",
    "        fdset = FaciesDataset(train_wells, test_wells=[test_well],\n",
    "                            features=[\"pseudoGR\"],\n",
    "                            pseudoGR_args={'features' : ['mean', 'var', 'p10', 'p90'], \n",
    "                                           'per_channel' : True},\n",
    "                            label_resolution=resolution)\n",
    "    \n",
    "        fdset.load_or_generate_data()\n",
    "    \n",
    "        # Build and train and re-load weights\n",
    "        wavenet_model_args['sequence_size'] = 32*128 // resolution\n",
    "        wavenet_model_args['network_args']['output_resolution'] = resolution\n",
    "    \n",
    "        chkpt_callback = ModelCheckpoint(CHKPT_FNAME, monitor='val_loss', verbose=1,\n",
    "                                        save_best_only=True, \n",
    "                                        save_weights_only=True)\n",
    "        wavenet_fit_args['callbacks'] = [chkpt_callback]\n",
    "    \n",
    "        wnet_model = NetworkModel(fdset, model_args=wavenet_model_args)\n",
    "        wnet_model.fit(fdset, **wavenet_fit_args)\n",
    "    \n",
    "        wnet_model.network.load_weights(CHKPT_FNAME)\n",
    "        os.remove(CHKPT_FNAME)\n",
    "    \n",
    "        # Get test results\n",
    "        results[test_well] = wnet_model.preds_dataframe(test_well)\n",
    "        results[test_well]['label_resolution'] = fdset.label_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base</th>\n",
       "      <th>confidence</th>\n",
       "      <th>depth</th>\n",
       "      <th>proba_0</th>\n",
       "      <th>proba_1</th>\n",
       "      <th>proba_2</th>\n",
       "      <th>proba_3</th>\n",
       "      <th>regression</th>\n",
       "      <th>top</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>...</th>\n",
       "      <th>Up10</th>\n",
       "      <th>Rp10</th>\n",
       "      <th>Gp10</th>\n",
       "      <th>Bp10</th>\n",
       "      <th>Up90</th>\n",
       "      <th>Rp90</th>\n",
       "      <th>Gp90</th>\n",
       "      <th>Bp90</th>\n",
       "      <th>label_resolution</th>\n",
       "      <th>well_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2208.128881</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.107317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.085920</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171624</td>\n",
       "      <td>0.174621</td>\n",
       "      <td>0.159884</td>\n",
       "      <td>0.319280</td>\n",
       "      <td>0.454419</td>\n",
       "      <td>0.467661</td>\n",
       "      <td>0.442424</td>\n",
       "      <td>0.450663</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2208.171841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.150277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.128881</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880047</td>\n",
       "      <td>0.942670</td>\n",
       "      <td>0.866832</td>\n",
       "      <td>0.909835</td>\n",
       "      <td>0.845520</td>\n",
       "      <td>0.864636</td>\n",
       "      <td>0.828564</td>\n",
       "      <td>0.876236</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2208.214801</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.193237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.171841</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.247755</td>\n",
       "      <td>1.342096</td>\n",
       "      <td>1.234849</td>\n",
       "      <td>1.233370</td>\n",
       "      <td>0.938230</td>\n",
       "      <td>0.990517</td>\n",
       "      <td>0.916740</td>\n",
       "      <td>0.933368</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2208.257761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.236197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.214801</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.368064</td>\n",
       "      <td>1.454332</td>\n",
       "      <td>1.358109</td>\n",
       "      <td>1.370884</td>\n",
       "      <td>1.032339</td>\n",
       "      <td>1.077094</td>\n",
       "      <td>1.009342</td>\n",
       "      <td>1.058081</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2208.300722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.279158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.257761</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.405945</td>\n",
       "      <td>1.483077</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.451311</td>\n",
       "      <td>1.007692</td>\n",
       "      <td>1.056017</td>\n",
       "      <td>0.983789</td>\n",
       "      <td>1.030519</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2208.343682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.322118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.300722</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927099</td>\n",
       "      <td>0.994784</td>\n",
       "      <td>0.914998</td>\n",
       "      <td>0.964090</td>\n",
       "      <td>1.007899</td>\n",
       "      <td>1.056400</td>\n",
       "      <td>0.984611</td>\n",
       "      <td>1.032468</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2208.386642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.365078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.343682</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.469747</td>\n",
       "      <td>1.541590</td>\n",
       "      <td>1.459162</td>\n",
       "      <td>1.541364</td>\n",
       "      <td>1.067968</td>\n",
       "      <td>1.113664</td>\n",
       "      <td>1.042961</td>\n",
       "      <td>1.102920</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2208.429602</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.408038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.386642</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.463327</td>\n",
       "      <td>1.535478</td>\n",
       "      <td>1.453492</td>\n",
       "      <td>1.532087</td>\n",
       "      <td>1.047413</td>\n",
       "      <td>1.095180</td>\n",
       "      <td>1.022811</td>\n",
       "      <td>1.079587</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2208.472563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.450998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.429602</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.395938</td>\n",
       "      <td>1.481629</td>\n",
       "      <td>1.384730</td>\n",
       "      <td>1.428362</td>\n",
       "      <td>1.001088</td>\n",
       "      <td>1.057979</td>\n",
       "      <td>0.975854</td>\n",
       "      <td>1.004307</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2208.515523</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.493959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.472563</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.381376</td>\n",
       "      <td>1.461845</td>\n",
       "      <td>1.369922</td>\n",
       "      <td>1.429185</td>\n",
       "      <td>1.021200</td>\n",
       "      <td>1.073437</td>\n",
       "      <td>0.996549</td>\n",
       "      <td>1.034043</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2208.558483</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.536919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.515523</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.343743</td>\n",
       "      <td>1.422208</td>\n",
       "      <td>1.332682</td>\n",
       "      <td>1.380162</td>\n",
       "      <td>1.024181</td>\n",
       "      <td>1.074385</td>\n",
       "      <td>1.000757</td>\n",
       "      <td>1.039846</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2208.601443</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.579879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.558483</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850562</td>\n",
       "      <td>0.976919</td>\n",
       "      <td>0.831901</td>\n",
       "      <td>0.713421</td>\n",
       "      <td>0.802427</td>\n",
       "      <td>0.876288</td>\n",
       "      <td>0.779679</td>\n",
       "      <td>0.728103</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2208.687364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.665800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.644403</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405780</td>\n",
       "      <td>0.407008</td>\n",
       "      <td>0.392850</td>\n",
       "      <td>0.541043</td>\n",
       "      <td>0.224847</td>\n",
       "      <td>0.226056</td>\n",
       "      <td>0.210783</td>\n",
       "      <td>0.288932</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2208.730324</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2208.708760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2208.687364</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.036832</td>\n",
       "      <td>1.084938</td>\n",
       "      <td>1.028066</td>\n",
       "      <td>1.109389</td>\n",
       "      <td>0.894846</td>\n",
       "      <td>0.923055</td>\n",
       "      <td>0.877799</td>\n",
       "      <td>0.904232</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2209.073838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.052274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.030878</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033749</td>\n",
       "      <td>0.027206</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.107324</td>\n",
       "      <td>0.345178</td>\n",
       "      <td>0.351538</td>\n",
       "      <td>0.334872</td>\n",
       "      <td>0.340140</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2209.116798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.095234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.073838</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768806</td>\n",
       "      <td>0.817728</td>\n",
       "      <td>0.758215</td>\n",
       "      <td>0.803097</td>\n",
       "      <td>0.480529</td>\n",
       "      <td>0.508469</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.450237</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2209.159758</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.138194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.116798</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.230718</td>\n",
       "      <td>1.310903</td>\n",
       "      <td>1.226302</td>\n",
       "      <td>1.174679</td>\n",
       "      <td>0.790078</td>\n",
       "      <td>0.817462</td>\n",
       "      <td>0.778076</td>\n",
       "      <td>0.753489</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2209.202719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.181155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.159758</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.330852</td>\n",
       "      <td>1.439683</td>\n",
       "      <td>1.326385</td>\n",
       "      <td>1.186509</td>\n",
       "      <td>0.860232</td>\n",
       "      <td>0.911095</td>\n",
       "      <td>0.843286</td>\n",
       "      <td>0.795187</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2209.245679</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.224115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.202719</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.372535</td>\n",
       "      <td>1.458284</td>\n",
       "      <td>1.369153</td>\n",
       "      <td>1.303474</td>\n",
       "      <td>0.867974</td>\n",
       "      <td>0.915367</td>\n",
       "      <td>0.850643</td>\n",
       "      <td>0.819538</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2209.288639</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.267075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.245679</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516321</td>\n",
       "      <td>0.564947</td>\n",
       "      <td>0.504948</td>\n",
       "      <td>0.518680</td>\n",
       "      <td>0.540188</td>\n",
       "      <td>0.555527</td>\n",
       "      <td>0.527181</td>\n",
       "      <td>0.543761</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2209.331599</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.310035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.288639</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262675</td>\n",
       "      <td>0.277114</td>\n",
       "      <td>0.248568</td>\n",
       "      <td>0.377218</td>\n",
       "      <td>0.113038</td>\n",
       "      <td>0.093609</td>\n",
       "      <td>0.102645</td>\n",
       "      <td>0.182428</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2209.374559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.352995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.331599</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274943</td>\n",
       "      <td>0.300483</td>\n",
       "      <td>0.262713</td>\n",
       "      <td>0.325795</td>\n",
       "      <td>0.050155</td>\n",
       "      <td>0.032571</td>\n",
       "      <td>0.040275</td>\n",
       "      <td>0.107252</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2209.417520</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.395956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.374559</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328437</td>\n",
       "      <td>0.364476</td>\n",
       "      <td>0.312322</td>\n",
       "      <td>0.396875</td>\n",
       "      <td>0.126475</td>\n",
       "      <td>0.113464</td>\n",
       "      <td>0.115527</td>\n",
       "      <td>0.194095</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2209.460480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.438916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.417520</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.596825</td>\n",
       "      <td>0.623782</td>\n",
       "      <td>0.586186</td>\n",
       "      <td>0.688421</td>\n",
       "      <td>0.453103</td>\n",
       "      <td>0.462201</td>\n",
       "      <td>0.439307</td>\n",
       "      <td>0.476771</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2209.503440</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.481876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.460480</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.392723</td>\n",
       "      <td>1.458801</td>\n",
       "      <td>1.386151</td>\n",
       "      <td>1.426813</td>\n",
       "      <td>0.883571</td>\n",
       "      <td>0.927011</td>\n",
       "      <td>0.864787</td>\n",
       "      <td>0.865656</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2209.546400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.524836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.503440</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.412377</td>\n",
       "      <td>1.500046</td>\n",
       "      <td>1.403338</td>\n",
       "      <td>1.399637</td>\n",
       "      <td>0.930963</td>\n",
       "      <td>0.987409</td>\n",
       "      <td>0.909351</td>\n",
       "      <td>0.901196</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2209.589361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.567797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.546400</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.468529</td>\n",
       "      <td>1.523782</td>\n",
       "      <td>1.464015</td>\n",
       "      <td>1.548716</td>\n",
       "      <td>0.949213</td>\n",
       "      <td>0.984666</td>\n",
       "      <td>0.929020</td>\n",
       "      <td>0.971143</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2209.632321</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.610757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.589361</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.427537</td>\n",
       "      <td>1.488006</td>\n",
       "      <td>1.421283</td>\n",
       "      <td>1.482310</td>\n",
       "      <td>0.924156</td>\n",
       "      <td>0.959142</td>\n",
       "      <td>0.904907</td>\n",
       "      <td>0.939597</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2209.675281</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.653717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.632321</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.303632</td>\n",
       "      <td>1.382077</td>\n",
       "      <td>1.294650</td>\n",
       "      <td>1.309193</td>\n",
       "      <td>0.908360</td>\n",
       "      <td>0.942487</td>\n",
       "      <td>0.888973</td>\n",
       "      <td>0.922980</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2209.718241</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2209.696677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2209.675281</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.329650</td>\n",
       "      <td>1.407192</td>\n",
       "      <td>1.322893</td>\n",
       "      <td>1.323563</td>\n",
       "      <td>0.916527</td>\n",
       "      <td>0.952078</td>\n",
       "      <td>0.897925</td>\n",
       "      <td>0.926738</td>\n",
       "      <td>256</td>\n",
       "      <td>204-19-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2314</th>\n",
       "      <td>2199.522739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2199.501175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2199.479778</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.201892</td>\n",
       "      <td>-1.057112</td>\n",
       "      <td>-1.245126</td>\n",
       "      <td>-1.311224</td>\n",
       "      <td>-0.295684</td>\n",
       "      <td>-0.169401</td>\n",
       "      <td>-0.312008</td>\n",
       "      <td>-0.495134</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2315</th>\n",
       "      <td>2199.565699</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2199.544135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2199.522739</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.795163</td>\n",
       "      <td>-0.579088</td>\n",
       "      <td>-0.848413</td>\n",
       "      <td>-1.104896</td>\n",
       "      <td>-0.282235</td>\n",
       "      <td>-0.152995</td>\n",
       "      <td>-0.297875</td>\n",
       "      <td>-0.502904</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>2199.608659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2199.587095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2199.565699</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.832663</td>\n",
       "      <td>-0.614912</td>\n",
       "      <td>-0.885639</td>\n",
       "      <td>-1.124023</td>\n",
       "      <td>-0.297588</td>\n",
       "      <td>-0.162814</td>\n",
       "      <td>-0.313776</td>\n",
       "      <td>-0.527887</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317</th>\n",
       "      <td>2199.651619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2199.630055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2199.608659</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.732171</td>\n",
       "      <td>-0.516218</td>\n",
       "      <td>-0.771279</td>\n",
       "      <td>-1.066836</td>\n",
       "      <td>-0.250834</td>\n",
       "      <td>-0.113871</td>\n",
       "      <td>-0.266340</td>\n",
       "      <td>-0.494158</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>2199.694580</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2199.673016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2199.651619</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.703668</td>\n",
       "      <td>-0.467702</td>\n",
       "      <td>-0.753283</td>\n",
       "      <td>-1.080117</td>\n",
       "      <td>-0.288315</td>\n",
       "      <td>-0.157813</td>\n",
       "      <td>-0.303426</td>\n",
       "      <td>-0.519005</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>2199.737540</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2199.715976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2199.694580</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.597673</td>\n",
       "      <td>-0.343401</td>\n",
       "      <td>-0.656328</td>\n",
       "      <td>-1.047852</td>\n",
       "      <td>-0.257763</td>\n",
       "      <td>-0.126616</td>\n",
       "      <td>-0.273553</td>\n",
       "      <td>-0.486905</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>2199.780500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2199.758936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2199.737540</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000533</td>\n",
       "      <td>-0.803629</td>\n",
       "      <td>-1.056368</td>\n",
       "      <td>-1.306393</td>\n",
       "      <td>-0.408003</td>\n",
       "      <td>-0.279894</td>\n",
       "      <td>-0.417907</td>\n",
       "      <td>-0.632371</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>2199.823460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2199.801896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2199.780500</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.483588</td>\n",
       "      <td>-0.228595</td>\n",
       "      <td>-0.539341</td>\n",
       "      <td>-0.948906</td>\n",
       "      <td>-0.248696</td>\n",
       "      <td>-0.114139</td>\n",
       "      <td>-0.263887</td>\n",
       "      <td>-0.503227</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>2199.866421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2199.844857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2199.823460</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015691</td>\n",
       "      <td>-0.840500</td>\n",
       "      <td>-1.065282</td>\n",
       "      <td>-1.244505</td>\n",
       "      <td>-0.324101</td>\n",
       "      <td>-0.196461</td>\n",
       "      <td>-0.336376</td>\n",
       "      <td>-0.559681</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>2199.909381</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2199.887817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2199.866421</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250452</td>\n",
       "      <td>-0.018631</td>\n",
       "      <td>-0.300692</td>\n",
       "      <td>-0.661367</td>\n",
       "      <td>-0.159017</td>\n",
       "      <td>-0.030607</td>\n",
       "      <td>-0.174766</td>\n",
       "      <td>-0.390931</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>2199.952341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2199.930777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2199.909381</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209324</td>\n",
       "      <td>0.034743</td>\n",
       "      <td>-0.259482</td>\n",
       "      <td>-0.649688</td>\n",
       "      <td>-0.193629</td>\n",
       "      <td>-0.061665</td>\n",
       "      <td>-0.209030</td>\n",
       "      <td>-0.436073</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>2200.124014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.102450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.081054</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414643</td>\n",
       "      <td>-0.189607</td>\n",
       "      <td>-0.461984</td>\n",
       "      <td>-0.854792</td>\n",
       "      <td>-0.304685</td>\n",
       "      <td>-0.176938</td>\n",
       "      <td>-0.316891</td>\n",
       "      <td>-0.569557</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>2200.166974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.145410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.124014</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.978529</td>\n",
       "      <td>-0.823869</td>\n",
       "      <td>-1.023549</td>\n",
       "      <td>-1.192161</td>\n",
       "      <td>-0.204996</td>\n",
       "      <td>-0.080331</td>\n",
       "      <td>-0.214998</td>\n",
       "      <td>-0.467018</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>2200.209935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.188371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.166974</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376330</td>\n",
       "      <td>-0.149051</td>\n",
       "      <td>-0.423700</td>\n",
       "      <td>-0.761510</td>\n",
       "      <td>1.108645</td>\n",
       "      <td>1.193888</td>\n",
       "      <td>1.101470</td>\n",
       "      <td>0.829144</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>2200.252895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.231331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.209935</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.090473</td>\n",
       "      <td>-2.065221</td>\n",
       "      <td>-2.131611</td>\n",
       "      <td>-1.947409</td>\n",
       "      <td>0.283261</td>\n",
       "      <td>0.395626</td>\n",
       "      <td>0.282211</td>\n",
       "      <td>0.033212</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>2200.295855</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.274291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.252895</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.121851</td>\n",
       "      <td>-0.985010</td>\n",
       "      <td>-1.168926</td>\n",
       "      <td>-1.227826</td>\n",
       "      <td>-0.150345</td>\n",
       "      <td>-0.030691</td>\n",
       "      <td>-0.162125</td>\n",
       "      <td>-0.382311</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>2200.338815</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.317251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.295855</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.998832</td>\n",
       "      <td>-1.947758</td>\n",
       "      <td>-2.041133</td>\n",
       "      <td>-1.763073</td>\n",
       "      <td>-0.612102</td>\n",
       "      <td>-0.521362</td>\n",
       "      <td>-0.617764</td>\n",
       "      <td>-0.750569</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>2200.381775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.360211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.338815</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.395364</td>\n",
       "      <td>-1.335186</td>\n",
       "      <td>-1.433505</td>\n",
       "      <td>-1.467773</td>\n",
       "      <td>0.162260</td>\n",
       "      <td>0.227849</td>\n",
       "      <td>0.155373</td>\n",
       "      <td>0.031734</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>2200.424736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.403172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.381775</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.921471</td>\n",
       "      <td>-1.894616</td>\n",
       "      <td>-1.958278</td>\n",
       "      <td>-1.967109</td>\n",
       "      <td>-0.104225</td>\n",
       "      <td>-0.003906</td>\n",
       "      <td>-0.114688</td>\n",
       "      <td>-0.297189</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>2200.467696</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.446132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.424736</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.698250</td>\n",
       "      <td>-1.657827</td>\n",
       "      <td>-1.735110</td>\n",
       "      <td>-1.738398</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>0.088511</td>\n",
       "      <td>-0.005498</td>\n",
       "      <td>-0.164122</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2334</th>\n",
       "      <td>2200.510656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.489092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.467696</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.683635</td>\n",
       "      <td>-0.609470</td>\n",
       "      <td>-0.713842</td>\n",
       "      <td>-0.791810</td>\n",
       "      <td>0.217034</td>\n",
       "      <td>0.278309</td>\n",
       "      <td>0.210184</td>\n",
       "      <td>0.104076</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>2200.553616</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.532052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.510656</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.997859</td>\n",
       "      <td>-1.995543</td>\n",
       "      <td>-2.034210</td>\n",
       "      <td>-1.970143</td>\n",
       "      <td>0.071470</td>\n",
       "      <td>0.137684</td>\n",
       "      <td>0.063469</td>\n",
       "      <td>-0.048225</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>2200.596577</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.575013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.553616</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.348255</td>\n",
       "      <td>-0.250794</td>\n",
       "      <td>-0.379385</td>\n",
       "      <td>-0.499987</td>\n",
       "      <td>0.085238</td>\n",
       "      <td>0.128002</td>\n",
       "      <td>0.081116</td>\n",
       "      <td>-0.008560</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>2200.639537</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.617973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.596577</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.701987</td>\n",
       "      <td>-1.675898</td>\n",
       "      <td>-1.737335</td>\n",
       "      <td>-1.727083</td>\n",
       "      <td>-0.791783</td>\n",
       "      <td>-0.783892</td>\n",
       "      <td>-0.785487</td>\n",
       "      <td>-0.816907</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>2200.682497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.660933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.639537</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.270113</td>\n",
       "      <td>-1.171314</td>\n",
       "      <td>-1.304120</td>\n",
       "      <td>-1.364544</td>\n",
       "      <td>0.122209</td>\n",
       "      <td>0.193076</td>\n",
       "      <td>0.114558</td>\n",
       "      <td>-0.023115</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>2200.725457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.703893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.682497</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.291813</td>\n",
       "      <td>-1.205772</td>\n",
       "      <td>-1.331853</td>\n",
       "      <td>-1.400326</td>\n",
       "      <td>0.065189</td>\n",
       "      <td>0.158318</td>\n",
       "      <td>0.056372</td>\n",
       "      <td>-0.116279</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>2200.768418</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.746853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.725457</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.665981</td>\n",
       "      <td>-1.578062</td>\n",
       "      <td>-1.708678</td>\n",
       "      <td>-1.749049</td>\n",
       "      <td>-0.051467</td>\n",
       "      <td>0.044975</td>\n",
       "      <td>-0.060531</td>\n",
       "      <td>-0.251520</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>2200.811378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.789814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.768418</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.523699</td>\n",
       "      <td>-1.440305</td>\n",
       "      <td>-1.564793</td>\n",
       "      <td>-1.653763</td>\n",
       "      <td>0.057275</td>\n",
       "      <td>0.138388</td>\n",
       "      <td>0.049866</td>\n",
       "      <td>-0.118088</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>2200.854338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.832774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.811378</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.196006</td>\n",
       "      <td>-1.095825</td>\n",
       "      <td>-1.237769</td>\n",
       "      <td>-1.300143</td>\n",
       "      <td>0.136267</td>\n",
       "      <td>0.204550</td>\n",
       "      <td>0.127661</td>\n",
       "      <td>0.010785</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>2200.897298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2200.875734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.854338</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.712367</td>\n",
       "      <td>-0.561600</td>\n",
       "      <td>-0.756299</td>\n",
       "      <td>-0.902734</td>\n",
       "      <td>0.229644</td>\n",
       "      <td>0.303546</td>\n",
       "      <td>0.220080</td>\n",
       "      <td>0.087313</td>\n",
       "      <td>256</td>\n",
       "      <td>204-20-6a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2344 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             base  confidence        depth  proba_0  proba_1  proba_2  \\\n",
       "0     2208.128881         1.0  2208.107317      0.0      0.0      1.0   \n",
       "1     2208.171841         1.0  2208.150277      0.0      0.0      1.0   \n",
       "2     2208.214801         1.0  2208.193237      0.0      0.0      1.0   \n",
       "3     2208.257761         1.0  2208.236197      0.0      0.0      1.0   \n",
       "4     2208.300722         1.0  2208.279158      0.0      0.0      1.0   \n",
       "5     2208.343682         1.0  2208.322118      0.0      0.0      1.0   \n",
       "6     2208.386642         1.0  2208.365078      0.0      0.0      1.0   \n",
       "7     2208.429602         1.0  2208.408038      0.0      0.0      1.0   \n",
       "8     2208.472563         1.0  2208.450998      0.0      0.0      1.0   \n",
       "9     2208.515523         1.0  2208.493959      0.0      0.0      1.0   \n",
       "10    2208.558483         1.0  2208.536919      0.0      0.0      1.0   \n",
       "11    2208.601443         1.0  2208.579879      0.0      0.0      1.0   \n",
       "12    2208.687364         1.0  2208.665800      0.0      0.0      1.0   \n",
       "13    2208.730324         1.0  2208.708760      0.0      0.0      1.0   \n",
       "14    2209.073838         1.0  2209.052274      0.0      0.0      1.0   \n",
       "15    2209.116798         1.0  2209.095234      0.0      0.0      1.0   \n",
       "16    2209.159758         1.0  2209.138194      0.0      0.0      1.0   \n",
       "17    2209.202719         1.0  2209.181155      0.0      0.0      1.0   \n",
       "18    2209.245679         1.0  2209.224115      0.0      0.0      1.0   \n",
       "19    2209.288639         1.0  2209.267075      0.0      0.0      1.0   \n",
       "20    2209.331599         1.0  2209.310035      0.0      0.0      1.0   \n",
       "21    2209.374559         1.0  2209.352995      0.0      0.0      1.0   \n",
       "22    2209.417520         1.0  2209.395956      0.0      0.0      1.0   \n",
       "23    2209.460480         1.0  2209.438916      0.0      0.0      1.0   \n",
       "24    2209.503440         1.0  2209.481876      0.0      0.0      1.0   \n",
       "25    2209.546400         1.0  2209.524836      0.0      0.0      1.0   \n",
       "26    2209.589361         1.0  2209.567797      0.0      0.0      1.0   \n",
       "27    2209.632321         1.0  2209.610757      0.0      0.0      1.0   \n",
       "28    2209.675281         1.0  2209.653717      0.0      0.0      1.0   \n",
       "29    2209.718241         1.0  2209.696677      0.0      0.0      1.0   \n",
       "...           ...         ...          ...      ...      ...      ...   \n",
       "2314  2199.522739         1.0  2199.501175      0.0      0.0      1.0   \n",
       "2315  2199.565699         1.0  2199.544135      0.0      0.0      1.0   \n",
       "2316  2199.608659         1.0  2199.587095      0.0      0.0      1.0   \n",
       "2317  2199.651619         1.0  2199.630055      0.0      0.0      1.0   \n",
       "2318  2199.694580         1.0  2199.673016      0.0      0.0      1.0   \n",
       "2319  2199.737540         1.0  2199.715976      0.0      0.0      1.0   \n",
       "2320  2199.780500         1.0  2199.758936      0.0      0.0      1.0   \n",
       "2321  2199.823460         1.0  2199.801896      0.0      0.0      1.0   \n",
       "2322  2199.866421         1.0  2199.844857      0.0      0.0      1.0   \n",
       "2323  2199.909381         1.0  2199.887817      0.0      0.0      1.0   \n",
       "2324  2199.952341         1.0  2199.930777      0.0      0.0      1.0   \n",
       "2325  2200.124014         1.0  2200.102450      0.0      0.0      1.0   \n",
       "2326  2200.166974         1.0  2200.145410      0.0      0.0      1.0   \n",
       "2327  2200.209935         1.0  2200.188371      0.0      0.0      1.0   \n",
       "2328  2200.252895         1.0  2200.231331      0.0      0.0      1.0   \n",
       "2329  2200.295855         1.0  2200.274291      0.0      0.0      1.0   \n",
       "2330  2200.338815         1.0  2200.317251      0.0      0.0      1.0   \n",
       "2331  2200.381775         1.0  2200.360211      0.0      0.0      1.0   \n",
       "2332  2200.424736         1.0  2200.403172      0.0      0.0      1.0   \n",
       "2333  2200.467696         1.0  2200.446132      0.0      0.0      1.0   \n",
       "2334  2200.510656         1.0  2200.489092      0.0      0.0      1.0   \n",
       "2335  2200.553616         1.0  2200.532052      0.0      0.0      1.0   \n",
       "2336  2200.596577         1.0  2200.575013      0.0      0.0      1.0   \n",
       "2337  2200.639537         1.0  2200.617973      0.0      0.0      1.0   \n",
       "2338  2200.682497         1.0  2200.660933      0.0      0.0      1.0   \n",
       "2339  2200.725457         1.0  2200.703893      0.0      0.0      1.0   \n",
       "2340  2200.768418         1.0  2200.746853      0.0      0.0      1.0   \n",
       "2341  2200.811378         1.0  2200.789814      0.0      0.0      1.0   \n",
       "2342  2200.854338         1.0  2200.832774      0.0      0.0      1.0   \n",
       "2343  2200.897298         1.0  2200.875734      0.0      0.0      1.0   \n",
       "\n",
       "      proba_3  regression          top  y_pred    ...          Up10      Rp10  \\\n",
       "0         0.0         2.0  2208.085920       2    ...      0.171624  0.174621   \n",
       "1         0.0         2.0  2208.128881       2    ...      0.880047  0.942670   \n",
       "2         0.0         2.0  2208.171841       2    ...      1.247755  1.342096   \n",
       "3         0.0         2.0  2208.214801       2    ...      1.368064  1.454332   \n",
       "4         0.0         2.0  2208.257761       2    ...      1.405945  1.483077   \n",
       "5         0.0         2.0  2208.300722       2    ...      0.927099  0.994784   \n",
       "6         0.0         2.0  2208.343682       2    ...      1.469747  1.541590   \n",
       "7         0.0         2.0  2208.386642       2    ...      1.463327  1.535478   \n",
       "8         0.0         2.0  2208.429602       2    ...      1.395938  1.481629   \n",
       "9         0.0         2.0  2208.472563       2    ...      1.381376  1.461845   \n",
       "10        0.0         2.0  2208.515523       2    ...      1.343743  1.422208   \n",
       "11        0.0         2.0  2208.558483       2    ...      0.850562  0.976919   \n",
       "12        0.0         2.0  2208.644403       2    ...      0.405780  0.407008   \n",
       "13        0.0         2.0  2208.687364       2    ...      1.036832  1.084938   \n",
       "14        0.0         2.0  2209.030878       2    ...      0.033749  0.027206   \n",
       "15        0.0         2.0  2209.073838       2    ...      0.768806  0.817728   \n",
       "16        0.0         2.0  2209.116798       2    ...      1.230718  1.310903   \n",
       "17        0.0         2.0  2209.159758       2    ...      1.330852  1.439683   \n",
       "18        0.0         2.0  2209.202719       2    ...      1.372535  1.458284   \n",
       "19        0.0         2.0  2209.245679       2    ...      0.516321  0.564947   \n",
       "20        0.0         2.0  2209.288639       2    ...      0.262675  0.277114   \n",
       "21        0.0         2.0  2209.331599       2    ...      0.274943  0.300483   \n",
       "22        0.0         2.0  2209.374559       2    ...      0.328437  0.364476   \n",
       "23        0.0         2.0  2209.417520       2    ...      0.596825  0.623782   \n",
       "24        0.0         2.0  2209.460480       2    ...      1.392723  1.458801   \n",
       "25        0.0         2.0  2209.503440       2    ...      1.412377  1.500046   \n",
       "26        0.0         2.0  2209.546400       2    ...      1.468529  1.523782   \n",
       "27        0.0         2.0  2209.589361       2    ...      1.427537  1.488006   \n",
       "28        0.0         2.0  2209.632321       2    ...      1.303632  1.382077   \n",
       "29        0.0         2.0  2209.675281       2    ...      1.329650  1.407192   \n",
       "...       ...         ...          ...     ...    ...           ...       ...   \n",
       "2314      0.0         2.0  2199.479778       2    ...     -1.201892 -1.057112   \n",
       "2315      0.0         2.0  2199.522739       2    ...     -0.795163 -0.579088   \n",
       "2316      0.0         2.0  2199.565699       2    ...     -0.832663 -0.614912   \n",
       "2317      0.0         2.0  2199.608659       2    ...     -0.732171 -0.516218   \n",
       "2318      0.0         2.0  2199.651619       2    ...     -0.703668 -0.467702   \n",
       "2319      0.0         2.0  2199.694580       2    ...     -0.597673 -0.343401   \n",
       "2320      0.0         2.0  2199.737540       2    ...     -1.000533 -0.803629   \n",
       "2321      0.0         2.0  2199.780500       2    ...     -0.483588 -0.228595   \n",
       "2322      0.0         2.0  2199.823460       2    ...     -1.015691 -0.840500   \n",
       "2323      0.0         2.0  2199.866421       2    ...     -0.250452 -0.018631   \n",
       "2324      0.0         2.0  2199.909381       2    ...     -0.209324  0.034743   \n",
       "2325      0.0         2.0  2200.081054       2    ...     -0.414643 -0.189607   \n",
       "2326      0.0         2.0  2200.124014       2    ...     -0.978529 -0.823869   \n",
       "2327      0.0         2.0  2200.166974       2    ...     -0.376330 -0.149051   \n",
       "2328      0.0         2.0  2200.209935       2    ...     -2.090473 -2.065221   \n",
       "2329      0.0         2.0  2200.252895       2    ...     -1.121851 -0.985010   \n",
       "2330      0.0         2.0  2200.295855       2    ...     -1.998832 -1.947758   \n",
       "2331      0.0         2.0  2200.338815       2    ...     -1.395364 -1.335186   \n",
       "2332      0.0         2.0  2200.381775       2    ...     -1.921471 -1.894616   \n",
       "2333      0.0         2.0  2200.424736       2    ...     -1.698250 -1.657827   \n",
       "2334      0.0         2.0  2200.467696       2    ...     -0.683635 -0.609470   \n",
       "2335      0.0         2.0  2200.510656       2    ...     -1.997859 -1.995543   \n",
       "2336      0.0         2.0  2200.553616       2    ...     -0.348255 -0.250794   \n",
       "2337      0.0         2.0  2200.596577       2    ...     -1.701987 -1.675898   \n",
       "2338      0.0         2.0  2200.639537       2    ...     -1.270113 -1.171314   \n",
       "2339      0.0         2.0  2200.682497       2    ...     -1.291813 -1.205772   \n",
       "2340      0.0         2.0  2200.725457       2    ...     -1.665981 -1.578062   \n",
       "2341      0.0         2.0  2200.768418       2    ...     -1.523699 -1.440305   \n",
       "2342      0.0         2.0  2200.811378       2    ...     -1.196006 -1.095825   \n",
       "2343      0.0         2.0  2200.854338       2    ...     -0.712367 -0.561600   \n",
       "\n",
       "          Gp10      Bp10      Up90      Rp90      Gp90      Bp90  \\\n",
       "0     0.159884  0.319280  0.454419  0.467661  0.442424  0.450663   \n",
       "1     0.866832  0.909835  0.845520  0.864636  0.828564  0.876236   \n",
       "2     1.234849  1.233370  0.938230  0.990517  0.916740  0.933368   \n",
       "3     1.358109  1.370884  1.032339  1.077094  1.009342  1.058081   \n",
       "4     1.395833  1.451311  1.007692  1.056017  0.983789  1.030519   \n",
       "5     0.914998  0.964090  1.007899  1.056400  0.984611  1.032468   \n",
       "6     1.459162  1.541364  1.067968  1.113664  1.042961  1.102920   \n",
       "7     1.453492  1.532087  1.047413  1.095180  1.022811  1.079587   \n",
       "8     1.384730  1.428362  1.001088  1.057979  0.975854  1.004307   \n",
       "9     1.369922  1.429185  1.021200  1.073437  0.996549  1.034043   \n",
       "10    1.332682  1.380162  1.024181  1.074385  1.000757  1.039846   \n",
       "11    0.831901  0.713421  0.802427  0.876288  0.779679  0.728103   \n",
       "12    0.392850  0.541043  0.224847  0.226056  0.210783  0.288932   \n",
       "13    1.028066  1.109389  0.894846  0.923055  0.877799  0.904232   \n",
       "14    0.026420  0.107324  0.345178  0.351538  0.334872  0.340140   \n",
       "15    0.758215  0.803097  0.480529  0.508469  0.467391  0.450237   \n",
       "16    1.226302  1.174679  0.790078  0.817462  0.778076  0.753489   \n",
       "17    1.326385  1.186509  0.860232  0.911095  0.843286  0.795187   \n",
       "18    1.369153  1.303474  0.867974  0.915367  0.850643  0.819538   \n",
       "19    0.504948  0.518680  0.540188  0.555527  0.527181  0.543761   \n",
       "20    0.248568  0.377218  0.113038  0.093609  0.102645  0.182428   \n",
       "21    0.262713  0.325795  0.050155  0.032571  0.040275  0.107252   \n",
       "22    0.312322  0.396875  0.126475  0.113464  0.115527  0.194095   \n",
       "23    0.586186  0.688421  0.453103  0.462201  0.439307  0.476771   \n",
       "24    1.386151  1.426813  0.883571  0.927011  0.864787  0.865656   \n",
       "25    1.403338  1.399637  0.930963  0.987409  0.909351  0.901196   \n",
       "26    1.464015  1.548716  0.949213  0.984666  0.929020  0.971143   \n",
       "27    1.421283  1.482310  0.924156  0.959142  0.904907  0.939597   \n",
       "28    1.294650  1.309193  0.908360  0.942487  0.888973  0.922980   \n",
       "29    1.322893  1.323563  0.916527  0.952078  0.897925  0.926738   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "2314 -1.245126 -1.311224 -0.295684 -0.169401 -0.312008 -0.495134   \n",
       "2315 -0.848413 -1.104896 -0.282235 -0.152995 -0.297875 -0.502904   \n",
       "2316 -0.885639 -1.124023 -0.297588 -0.162814 -0.313776 -0.527887   \n",
       "2317 -0.771279 -1.066836 -0.250834 -0.113871 -0.266340 -0.494158   \n",
       "2318 -0.753283 -1.080117 -0.288315 -0.157813 -0.303426 -0.519005   \n",
       "2319 -0.656328 -1.047852 -0.257763 -0.126616 -0.273553 -0.486905   \n",
       "2320 -1.056368 -1.306393 -0.408003 -0.279894 -0.417907 -0.632371   \n",
       "2321 -0.539341 -0.948906 -0.248696 -0.114139 -0.263887 -0.503227   \n",
       "2322 -1.065282 -1.244505 -0.324101 -0.196461 -0.336376 -0.559681   \n",
       "2323 -0.300692 -0.661367 -0.159017 -0.030607 -0.174766 -0.390931   \n",
       "2324 -0.259482 -0.649688 -0.193629 -0.061665 -0.209030 -0.436073   \n",
       "2325 -0.461984 -0.854792 -0.304685 -0.176938 -0.316891 -0.569557   \n",
       "2326 -1.023549 -1.192161 -0.204996 -0.080331 -0.214998 -0.467018   \n",
       "2327 -0.423700 -0.761510  1.108645  1.193888  1.101470  0.829144   \n",
       "2328 -2.131611 -1.947409  0.283261  0.395626  0.282211  0.033212   \n",
       "2329 -1.168926 -1.227826 -0.150345 -0.030691 -0.162125 -0.382311   \n",
       "2330 -2.041133 -1.763073 -0.612102 -0.521362 -0.617764 -0.750569   \n",
       "2331 -1.433505 -1.467773  0.162260  0.227849  0.155373  0.031734   \n",
       "2332 -1.958278 -1.967109 -0.104225 -0.003906 -0.114688 -0.297189   \n",
       "2333 -1.735110 -1.738398  0.003612  0.088511 -0.005498 -0.164122   \n",
       "2334 -0.713842 -0.791810  0.217034  0.278309  0.210184  0.104076   \n",
       "2335 -2.034210 -1.970143  0.071470  0.137684  0.063469 -0.048225   \n",
       "2336 -0.379385 -0.499987  0.085238  0.128002  0.081116 -0.008560   \n",
       "2337 -1.737335 -1.727083 -0.791783 -0.783892 -0.785487 -0.816907   \n",
       "2338 -1.304120 -1.364544  0.122209  0.193076  0.114558 -0.023115   \n",
       "2339 -1.331853 -1.400326  0.065189  0.158318  0.056372 -0.116279   \n",
       "2340 -1.708678 -1.749049 -0.051467  0.044975 -0.060531 -0.251520   \n",
       "2341 -1.564793 -1.653763  0.057275  0.138388  0.049866 -0.118088   \n",
       "2342 -1.237769 -1.300143  0.136267  0.204550  0.127661  0.010785   \n",
       "2343 -0.756299 -0.902734  0.229644  0.303546  0.220080  0.087313   \n",
       "\n",
       "      label_resolution  well_name  \n",
       "0                  256   204-19-6  \n",
       "1                  256   204-19-6  \n",
       "2                  256   204-19-6  \n",
       "3                  256   204-19-6  \n",
       "4                  256   204-19-6  \n",
       "5                  256   204-19-6  \n",
       "6                  256   204-19-6  \n",
       "7                  256   204-19-6  \n",
       "8                  256   204-19-6  \n",
       "9                  256   204-19-6  \n",
       "10                 256   204-19-6  \n",
       "11                 256   204-19-6  \n",
       "12                 256   204-19-6  \n",
       "13                 256   204-19-6  \n",
       "14                 256   204-19-6  \n",
       "15                 256   204-19-6  \n",
       "16                 256   204-19-6  \n",
       "17                 256   204-19-6  \n",
       "18                 256   204-19-6  \n",
       "19                 256   204-19-6  \n",
       "20                 256   204-19-6  \n",
       "21                 256   204-19-6  \n",
       "22                 256   204-19-6  \n",
       "23                 256   204-19-6  \n",
       "24                 256   204-19-6  \n",
       "25                 256   204-19-6  \n",
       "26                 256   204-19-6  \n",
       "27                 256   204-19-6  \n",
       "28                 256   204-19-6  \n",
       "29                 256   204-19-6  \n",
       "...                ...        ...  \n",
       "2314               256  204-20-6a  \n",
       "2315               256  204-20-6a  \n",
       "2316               256  204-20-6a  \n",
       "2317               256  204-20-6a  \n",
       "2318               256  204-20-6a  \n",
       "2319               256  204-20-6a  \n",
       "2320               256  204-20-6a  \n",
       "2321               256  204-20-6a  \n",
       "2322               256  204-20-6a  \n",
       "2323               256  204-20-6a  \n",
       "2324               256  204-20-6a  \n",
       "2325               256  204-20-6a  \n",
       "2326               256  204-20-6a  \n",
       "2327               256  204-20-6a  \n",
       "2328               256  204-20-6a  \n",
       "2329               256  204-20-6a  \n",
       "2330               256  204-20-6a  \n",
       "2331               256  204-20-6a  \n",
       "2332               256  204-20-6a  \n",
       "2333               256  204-20-6a  \n",
       "2334               256  204-20-6a  \n",
       "2335               256  204-20-6a  \n",
       "2336               256  204-20-6a  \n",
       "2337               256  204-20-6a  \n",
       "2338               256  204-20-6a  \n",
       "2339               256  204-20-6a  \n",
       "2340               256  204-20-6a  \n",
       "2341               256  204-20-6a  \n",
       "2342               256  204-20-6a  \n",
       "2343               256  204-20-6a  \n",
       "\n",
       "[2344 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "for well_name, results_df in results.items():\n",
    "    results_df['well_name'] = well_name\n",
    "    dfs.append(results_df)\n",
    "    \n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('wavenet_multi_resolution_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<matplotlib.axes._subplots.AxesSubplot object at 0x7ef6e65f8ac8>\n",
      "  <matplotlib.axes._subplots.AxesSubplot object at 0x7ef723ce9f28>]\n",
      " [<matplotlib.axes._subplots.AxesSubplot object at 0x7ef723c97278>\n",
      "  <matplotlib.axes._subplots.AxesSubplot object at 0x7ef723cbc588>]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "axes = df.regression.hist(by=df.y_true, figsize=(15,15), alpha=0.4)\n",
    "print(axes)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    df.regression[df.y_pred==i].hist(ax=ax, color='blue', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204-19-6\n",
      "204-20-1Z\n",
      "204-20-6a\n",
      "204-24a-6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "axes = df.y_true.hist(by=df.well_name, figsize=(15,15), alpha=0.4)\n",
    "\n",
    "well_names = [\"204-19-6\", \"204-20-1Z\", \"204-20-6a\", \"204-24a-6\", \"205-21b-3\"]\n",
    "\n",
    "for i, (ax, name) in enumerate(zip(axes.flat, well_names)):\n",
    "    print(name)\n",
    "    df.y_pred[df.well_name==name].hist(ax=ax, color='blue', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdset.well_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdset.test_well_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(30).reshape(5, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(30).reshape(5, 3, 2).reshape(-1, 2).reshape(5, 3 ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
